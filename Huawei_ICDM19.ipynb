{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy.random import randint\n",
    "#!pip install git+git://github.com/ivannz/trmf\n",
    "from functools import reduce\n",
    "import math\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from trmf import TRMFRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error as mse_score\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.base import clone\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "#simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Use this file to feed into the clusterwise regression experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = 'Export/ICDM19/'\n",
    "#Train\n",
    "import_ = 'import/huawei/ensemble_clusterings_ff_huawei.csv'\n",
    "train = 'import/huawei/huawei_train.csv'\n",
    "test = 'import/huawei/huawei_test.csv'\n",
    "#import_test_yearly = 'import/M4/Yearly-test.csv'#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 ms, sys: 0 ns, total: 48 ms\n",
      "Wall time: 46.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv(train)\n",
    "clusters = pd.read_csv(import_).drop(columns ='Unnamed: 0')\n",
    "test_df = pd.read_csv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take only with future flag = 0\n",
    "test_df = test_df.loc[test_df['Unnamed: 0']==str(0.0)].drop(columns ='Unnamed: 0').fillna(0).reset_index().drop(columns = 'index')\n",
    "train_df = train_df.loc[train_df['Unnamed: 0']==str(0.0)].drop(columns ='Unnamed: 0').fillna(0).reset_index().drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add clusters\n",
    "train_df.insert(1,'GMM_Pair', clusters.GMM_Pair_ff0)\n",
    "train_df.insert(1,'GMM_Voting' , clusters.GMM_Voting_ff0)\n",
    "train_df.insert(1,'random initialization1',randint(0,max(clusters.GMM_Pair_ff0)-1,train_df.shape[0]))\n",
    "train_df.insert(1,'random initialization2',randint(0,max(clusters.GMM_Voting_ff0)-1,train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Do hard clusterwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = TRMFRegressor(n_components=2,\n",
    "                 n_order=2,\n",
    "                 fit_regression=False,\n",
    "                 fit_intercept=True,\n",
    "                 nonnegative_factors=True,\n",
    "                 n_max_mf_iter=5)\n",
    "\n",
    "\n",
    "def gridsearch_trmf(X,test_targets):\n",
    "    grid = ParameterGrid(dict(\n",
    "    n_components=np.r_[1:2],#17],\n",
    "    n_order=np.r_[1:2],#17],\n",
    "    C_Z=np.logspace(-2, 1,num=1),#,num=4),\n",
    "    C_F=np.logspace(-2, 1,num=1),#,num=4),\n",
    "    C_phi=np.logspace(-2, 1,num=1),#,num=4),\n",
    "    eta_Z=np.linspace(0.05, 0.95,num=1)#,num=10),\n",
    "    ))\n",
    "    \n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = transformer.fit_transform(X)\n",
    "\n",
    "    \n",
    "    # run the experiment in parallel\n",
    "    par_ = Parallel(n_jobs=-1, verbose=0)\n",
    "    results = par_(delayed(helper)(par, X, test_targets.transpose(),transformer) for par in grid)\n",
    "    \n",
    "    keys = ['n_order', 'n_components', 'eta_Z', 'C_phi', 'C_Z', 'C_F']\n",
    "    data = dict((tuple(par[k] for k in keys), rmse,) for par, rmse in results)\n",
    "    sr = pd.Series(data, name=\"rmse\").sort_index().rename_axis(keys)\n",
    "    cube = sr.values.reshape(*[len(grid.param_grid[0][k]) for k in keys])\n",
    "    stepping = [grid.param_grid[0][k] for k in keys]\n",
    "    # find the flat index of the smallest value\n",
    "    flat_index = np.argmin(cube)\n",
    "    # ... and unravel into into a multidimensional index\n",
    "    index = np.unravel_index(flat_index, cube.shape)\n",
    "    # collect the best paramaters from the grid\n",
    "    best_ = {k: grid.param_grid[0][k][i] for k, i in zip(keys, index)}\n",
    "    return best_\n",
    "\n",
    "transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "def helper(par, train, test,  transformer,base=base):\n",
    "    basee = TRMFRegressor(n_components=1,\n",
    "                     n_order=0,\n",
    "                     fit_regression=False,\n",
    "                     fit_intercept=True,\n",
    "                     nonnegative_factors=True,\n",
    "                     n_max_mf_iter=5)\n",
    "    \n",
    "    # clone, set parameters and fit\n",
    "    trmf = clone(basee).set_params(**par).fit(train)\n",
    "    \n",
    "    # predict and return\n",
    "    pred = transformer.inverse_transform(\n",
    "        trmf.predict(n_ahead=len(test)))\n",
    "\n",
    "    return par, mean_squared_error(test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true,y_pred,training_series= None): \n",
    "    weight = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    output = np.divide(np.abs(y_true - y_pred), weight,\n",
    "    where=weight > 0,\n",
    "    out=np.full_like(weight, np.nan))\n",
    "    return np.mean(np.nan_to_num(output))\n",
    "    #if ((np.abs(A) + np.abs(F))==0):\n",
    "   #     return 0\n",
    "   # if math.isnan((100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F))))):\n",
    "   #     return 0\n",
    " #   scr  = 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "   # return np.nan_to_num(scr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinball_loss(A,F,training_series= None,tau = 0.49):\n",
    "    return np.mean(np.maximum(F - A, 0) * (1 - tau) +\n",
    "                 np.maximum(F - A, 0) * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred,training_series= None): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(testing_series, prediction_series,training_series):\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterwise_regression_sklearn(X,Y,regr,regr_gridsearch_params,\n",
    "                           cluster_column ='supercluster',\n",
    "                            horizon = True,\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           recalculate_assignment = 100,\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Runs clusterwise regression for sklearn functions\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    cluster_column : column with given clusters\n",
    "    horizon = :  True  - 1-step horizon (1 can be modified), False - n periods ahead\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    gridsearch_limit : # of times to run GridSeach for each cluster (>=1)\n",
    "    recalculate_assignment : # of assignments after which the clusters will be recalculated\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    clusters : Dictionary with trained models for each cluster\n",
    "    old_cluster_for_user : Dictionary with optimal cluster for each user (after moving)\n",
    "    cluster_parameters_for_gridsearch : Dictionary with optimal parameters for each cluster model\n",
    "    weights\n",
    "    \"\"\"   \n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()  \n",
    "    n_ahead = test_targets.shape[1]\n",
    "    clusters = {}\n",
    "    # save cluster info for clusterwise regression\n",
    "    cluster_error = {}\n",
    "    cluster_has_changed = {}\n",
    "    cluster_data_X = {}\n",
    "    counter_for_gridsearch = 0\n",
    "    old_clusters ={}\n",
    "    cluster_data_true_values = {}\n",
    "    cluster_predicted = {}\n",
    "    score_for_user = {}\n",
    "    cluster_parameters_for_gridsearch = {}\n",
    "    previous_total_error = math.inf\n",
    "    limit = 50\n",
    "    for i in list(range(0,limit+1)): # Some limit on iterations just in case\n",
    "        # Assign user to clusters\n",
    "        cluster_for_user = {}\n",
    "        cluster_for_user  = train_targets[cluster_column].to_dict()\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            # Save training data for this cluster\n",
    "            \n",
    "            cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "            cluster_data_true_values[cluster_number] = test_targets.loc[ cluster_data_X[cluster_number].index]\n",
    "            cluster_predicted[cluster_number] =pd.DataFrame()\n",
    "            if (cluster_number not in cluster_has_changed) or (cluster_has_changed[cluster_number]== True):\n",
    "                    clusters[cluster_number] = clone(regr)\n",
    "                    cluster_predicted[cluster_number] =pd.DataFrame()\n",
    "            for slide in list(range(0,test_targets.shape[1])):\n",
    "                #Train\n",
    "                transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "                #print(\"X_train = \",cluster_data_X[cluster_number].iloc[:,:-1].columns)\n",
    "                fitted_ = transformer.fit_transform(cluster_data_X[cluster_number].iloc[:,:-1]) # X - all col except last one\n",
    "                #print(\"y_train = \",cluster_data_X[cluster_number].iloc[:,-1:].columns)\n",
    "                clusters[cluster_number].fit(fitted_,cluster_data_X[cluster_number].iloc[:,-1:]) # y - last column\n",
    "                #Test\n",
    "                #delete first col\n",
    "                cluster_data_X[cluster_number] = cluster_data_X[cluster_number].drop(cluster_data_X[cluster_number].columns[0], axis=1)  \n",
    "                #print(\"X_test = \",cluster_data_X[cluster_number].columns)\n",
    "                fitted_ = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "                cluster_predicted[cluster_number][slide] =clusters[cluster_number].predict(fitted_).ravel() \n",
    "                #add new one at the end (what we've just predicted)\n",
    "                if (horizon == False):\n",
    "                    cluster_data_X[cluster_number][slide] = cluster_predicted[cluster_number][slide].values \n",
    "                else:\n",
    "                    cluster_data_X[cluster_number][slide] =(cluster_data_true_values[cluster_number].iloc[:,slide])   # true values\n",
    "                #print(\"y_true = \",cluster_data_true_values[cluster_number].columns[slide])\n",
    "           # return cluster_predicted           \n",
    "            # Save prediction\n",
    "            cluster_predicted[cluster_number] = pd.DataFrame(cluster_predicted[cluster_number])\n",
    "\n",
    "            cluster_predicted[cluster_number].columns  = cluster_data_true_values[cluster_number].columns \n",
    "            cluster_predicted[cluster_number].index = cluster_data_true_values[cluster_number].index\n",
    "            #save cluster score\n",
    "            cluster_error[cluster_number] =  score_function(cluster_data_true_values[cluster_number].values.ravel(),cluster_predicted[cluster_number].values.ravel())\n",
    "        print(\"Error per cluster: \",cluster_error)\n",
    "        # flatten true values\n",
    "        all_true_values  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_data_true_values.values())  ]])   )\n",
    "        # flatten prediction \n",
    "        all_predictions  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_predicted.values())  ]])   )\n",
    "        total_error =  score_function(np.array(all_true_values),np.array(all_predictions))\n",
    "        print(\"New error: \",round(total_error,4))\n",
    "        # If new score is worse - load last cluster and run final prediction.\n",
    "        if (total_error > previous_total_error):\n",
    "            if counter_iterations ==3:\n",
    "               # train_targets[cluster_column] = list(old_cluster_for_user.values())\n",
    "                print('Error has increased. Backtrack to previous iteration')\n",
    "                return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i-counter_iterations\n",
    "            counter_iterations+=1\n",
    "        else:\n",
    "            counter_iterations = 0 \n",
    "            previous_total_error = total_error\n",
    "            old_clusters = clusters.copy()\n",
    "            old_cluster_for_user = cluster_for_user.copy()\n",
    "       \n",
    "        if (i>=limit):# return prev iteration\n",
    "            return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i\n",
    "        # Calculate score per user\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():\n",
    "                score_for_user[index] = score_function(row, cluster_predicted[cluster_number].loc[index,:])\n",
    "        # Assume that we haven't change any cluster\n",
    "        for x in cluster_has_changed:\n",
    "            cluster_has_changed[x]=False        \n",
    "        cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "       # old_clusters = {}\n",
    "       # old_cluster_for_user = {}\n",
    "        ii =0\n",
    "        reassignment_count = 0 \n",
    "        if (i==0):\n",
    "            old_clusters = clusters.copy()\n",
    "        #Take observation x from cluster a, where error_x > error_a\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            # All ids in this cluster are already in cluster with lowest possible mae\n",
    "            if (cluster_number == cluster_lowest_mae):\n",
    "                continue\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():    \n",
    "            # User is in current cluster -> we can reassign him\n",
    "                if (cluster_number  == cluster_for_user[index]):\n",
    "                    if (score_for_user[index] > cluster_error[cluster_number]):\n",
    "                        ii+=1#counter\n",
    "                        # Have to refit changed clusters             \n",
    "                        cluster_has_changed[cluster_number] = True\n",
    "                        cluster_has_changed[cluster_lowest_mae] = True\n",
    "                        # Move observation x to the cluster with the lowest training erro\n",
    "                        train_targets.loc[index,cluster_column] = cluster_lowest_mae\n",
    "                        reassignment_count+=1\n",
    "                        if reassignment_count==recalculate_assignment:\n",
    "                            reassignment_count=0\n",
    "                            \n",
    "                            # RECALCULATION - only best performing cluster (because)- TODO:save snapshot of prev cluster for backtracking\n",
    "                            cluster_data_X_reassigned =  train_targets.loc[train_targets[cluster_column] == cluster_lowest_mae].drop(columns =columns_wo_TS)\n",
    "                            cluster_data_true_values_reassigned= test_targets.loc[ cluster_data_X_reassigned.index]\n",
    "                           \n",
    "                            clusters_reassignemnt = clone(clusters[cluster_lowest_mae])\n",
    "                            cluster_predicted_reassigned = pd.DataFrame()\n",
    "                            for slide in list(range(0,test_targets.shape[1])):\n",
    "                                #Train\n",
    "                                transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "       \n",
    "                                fitted_ = transformer.fit_transform(cluster_data_X_reassigned.iloc[:,:-1]) # X - all col except last one\n",
    "                                clusters_reassignemnt.fit(fitted_,cluster_data_X_reassigned.iloc[:,-1:]) # y - last column\n",
    "                                #Test\n",
    "                                #delete first col\n",
    "                                cluster_data_X_reassigned = cluster_data_X_reassigned.drop(cluster_data_X_reassigned.columns[0], axis=1)  \n",
    "                                predd = transformer.fit_transform(cluster_data_X_reassigned)\n",
    "                                cluster_predicted_reassigned[slide] =clusters[cluster_number].predict(predd).ravel() \n",
    "                                #add new one at the end (what we've just predicted)\n",
    "                                if horizon == False:\n",
    "                                    cluster_data_X_reassigned[slide] = cluster_predicted_reassigned[slide].values\n",
    "                                else:\n",
    "                                    cluster_data_X_reassigned[slide] =(cluster_data_true_values_reassigned.iloc[:,slide]) \n",
    "                            # Save prediction\n",
    "                            cluster_predicted_reassigned = pd.DataFrame(cluster_predicted_reassigned)\n",
    "                            cluster_error[cluster_lowest_mae] = score_function(cluster_data_true_values_reassigned.values.ravel(),cluster_predicted_reassigned.values.ravel())\n",
    "                            cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "        if (i==0):\n",
    "            old_cluster_for_user = cluster_for_user.copy() # as well as save last assigned clusters                  \n",
    "        print('Moved ', ii, ' users to another cluster \\n')   \n",
    "   # return old_clusters,old_cluster_for_user,cluster_parameters_for_gridsearch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prediction_after_clusterwise_reg_sklearn(\n",
    "                           X,Y,regr,\n",
    "    dataset,\n",
    "    method,\n",
    "    frequency,\n",
    "    model,\n",
    "                            horizon= True,\n",
    "                           step = 'unstated',\n",
    "                           clusters = None,\n",
    "                           clusters_parameters = None,\n",
    "                           new_clusters_for_users=None,\n",
    "                           cluster_column ='supercluster',\n",
    "                           scores_per_cluster = True,\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                           gridsearch_use = False,):\n",
    "    \"\"\"\n",
    "    Performs final prediction and generates DataFrame for Alibaba Data\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    horizon : stated previously\n",
    "    step : 'before_classifier' or 'after_classifier', for report generation\n",
    "    clusters: dictionary with models for each cluster. If None, then refit\n",
    "    clusters_parameters: dictionary with parameters for each cluster model\n",
    "    new_clusters_for_users: dictionary with clusters for each user. \n",
    "    cluster_column : column with given clusters\n",
    "    \n",
    "    scores_per_cluster : if False the score will be given for the all dataset, not by cluster\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    gridsearch_use : Run gridSeach (True) or use clusters_parameters (False)?\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    result : DataFrame with columns\n",
    "    cluster, user id, true target, prediction, rmse, model, method (all data, clustered), step (all data, before classifier, after classifier)\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()\n",
    "    n_ahead = test_targets.shape[1]\n",
    "    if (new_clusters_for_users is not None):\n",
    "        # we assume that the order in DataFrame is the same as in clusterwise regression\n",
    "        train_targets[cluster_column]=list(cluster_per_user.values()) \n",
    "    cluster_data_X = {}\n",
    "    cluster_data_true_values = {}\n",
    "    if (clusters_parameters == None or gridsearch_use==True):\n",
    "        clusters_parameters = {}\n",
    "    if (clusters == None or clusters_parameters ==None):\n",
    "        clusters = {}\n",
    "    for cluster_number in train_targets[cluster_column].unique():\n",
    "        cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "        cluster_data_true_values[cluster_number] = test_targets.iloc[ cluster_data_X[cluster_number].index]\n",
    "        cluster_data_true_values[cluster_number].index =   test_targets.iloc[ cluster_data_X[cluster_number].index].index\n",
    "        predicted =pd.DataFrame()\n",
    "        \n",
    "        if (cluster_number not in clusters or ( cluster_number not in  clusters_parameters) or gridsearch_use==True):\n",
    "            clusters[cluster_number] = clone(regr)\n",
    "            \n",
    "        for slide in list(range(0,test_targets.shape[1])):\n",
    "            #Train\n",
    "            transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "            #print(\"X_train = \",cluster_data_X[cluster_number].iloc[:,:-1].columns)\n",
    "            fitted_ = transformer.fit_transform(cluster_data_X[cluster_number].iloc[:,:-1]) # X - all col except last one\n",
    "            #print(\"y_train = \",cluster_data_X[cluster_number].iloc[:,-1:].columns)\n",
    "           # print(cluster_data_X[cluster_number].iloc[:,-1:])\n",
    "            clusters[cluster_number].fit(fitted_,cluster_data_X[cluster_number].iloc[:,-1:]) # y - last column\n",
    "            #Test\n",
    "            #delete first col\n",
    "            cluster_data_X[cluster_number] = cluster_data_X[cluster_number].drop(cluster_data_X[cluster_number].columns[0], axis=1)  \n",
    "            #print(\"X_test = \",cluster_data_X[cluster_number].columns)\n",
    "            fitted_ = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "            predicted[slide] =clusters[cluster_number].predict(fitted_).ravel() \n",
    "            #add new one at the end (what we've just predicted)\n",
    "            if (horizon == False):\n",
    "                cluster_data_X[cluster_number][slide] = predicted[slide].values \n",
    "            else:\n",
    "              #  print(cluster_data_true_values[cluster_number].iloc[:,slide])\n",
    "                cluster_data_X[cluster_number][slide] =(cluster_data_true_values[cluster_number].iloc[:,slide]).values   # true values\n",
    "        \n",
    "        \n",
    "        df_prediction = pd.DataFrame(predicted)\n",
    "        df_prediction.columns  = cluster_data_true_values[cluster_number].columns \n",
    "        df_prediction.index = cluster_data_true_values[cluster_number].index\n",
    "       # df_prediction.index =  train_targets.loc[train_targets[cluster_column] == cluster_number][\"Unnamed: 1\"]\n",
    "        # Transform prediction for output\n",
    "        #MODEL        \n",
    "        \n",
    "        df_prediction.insert(0,'Score',score_function(cluster_data_true_values[cluster_number].values.ravel(),df_prediction.values.ravel(),training_series=fitted_))\n",
    "        df_prediction.insert(0,'model',model)\n",
    "        #Dataset\n",
    "        df_prediction.insert(0,'dataset',dataset)\n",
    "        #METHOD\n",
    "        df_prediction.insert(0,'method',method)\n",
    "        #STEP\n",
    "        df_prediction.insert(0,'frequency',frequency)\n",
    "        if horizon == True:\n",
    "            df_prediction.insert(0,'horizon',str(1)+'_step')\n",
    "        else:\n",
    "            df_prediction.insert(0,'horizon','full')\n",
    "        # CLUSTER\n",
    "        df_prediction.insert(0,'cluster_id',str(cluster_column)+'_'+str(cluster_number))\n",
    "        #Wide to logn\n",
    "        #print(pd.DataFrame(df_prediction))\n",
    "        df_prediction = df_prediction.reset_index().melt(id_vars =['Unnamed: 1','cluster_id','model','horizon','dataset','method','frequency','Score']).rename(index = str, columns={'variable':'date','value':'predicted_value','Unnamed: 1':'id'})\n",
    "        true_values=cluster_data_true_values[cluster_number].reset_index().melt(id_vars=['Unnamed: 1']).rename(index = str, columns={'variable':'date','value':'true_value','Unnamed: 1':'id'})\n",
    "        result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))\n",
    "    if scores_per_cluster == False:\n",
    "           result.Score = score_function(result.true_value,result.predicted_value,\n",
    "                             training_series=train_targets.drop(columns =columns_wo_TS).values.ravel())\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterwise_regression(X,Y,\n",
    "                           cluster_column ='supercluster',\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           gridsearch_limit = 1,\n",
    "                           recalculate_assignment = 100,\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Runs clusterwise regression\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    cluster_column : column with given clusters\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    gridsearch_limit : # of times to run GridSeach for each cluster (>=1)\n",
    "    recalculate_assignment : # of assignments after which the clusters will be recalculated\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    clusters : Dictionary with trained models for each cluster\n",
    "    old_cluster_for_user : Dictionary with optimal cluster for each user (after moving)\n",
    "    cluster_parameters_for_gridsearch : Dictionary with optimal parameters for each cluster model\n",
    "    weights\n",
    "    \"\"\"\n",
    "        \n",
    "        \n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()  # test_Y \n",
    "    n_ahead = test_targets.shape[1]\n",
    "    clusters = {}\n",
    "    # save cluster info for clusterwise regression\n",
    "    cluster_error = {}\n",
    "    cluster_has_changed = {}\n",
    "    cluster_data_X = {}\n",
    "    counter_for_gridsearch = 0\n",
    "    old_clusters ={}\n",
    "    counter_iterations = 0 \n",
    "    cluster_data_true_values = {}\n",
    "    cluster_predicted = {}\n",
    "    score_for_user = {}\n",
    "    cluster_parameters_for_gridsearch = {}\n",
    "    previous_total_error = math.inf\n",
    "    limit = 50\n",
    "    for i in list(range(0,limit+1)): # Some limit on iterations just in case\n",
    "        # Assign user to clusters\n",
    "        cluster_for_user = {}\n",
    "        cluster_for_user  = train_targets[cluster_column].to_dict()\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            cluster_predicted[cluster_number] =pd.DataFrame()\n",
    "            # Save training data for this cluster\n",
    "            cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "            cluster_data_true_values[cluster_number] = test_targets.loc[ cluster_data_X[cluster_number].index]\n",
    "            # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "            transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "            cluster_data_X[cluster_number] = cluster_data_X[cluster_number].transpose()\n",
    "           # print(cluster_number)\n",
    "           # print(cluster_data_X[cluster_number])\n",
    "            cluster_data_X[cluster_number] = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "            #GridSearchCV for TRMF\n",
    "            # If initial initialization or cluster has changed\n",
    "            if (cluster_number not in cluster_has_changed) or (cluster_has_changed[cluster_number]== True):\n",
    "                if (i<gridsearch_limit):\n",
    "                    # add prev_cluster_paraeters\n",
    "                    cluster_parameters_for_gridsearch[cluster_number] = gridsearch_trmf(cluster_data_X[cluster_number],cluster_data_true_values[cluster_number])\n",
    "                clusters[cluster_number ]= TRMFRegressor(**cluster_parameters_for_gridsearch[cluster_number], eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                                 fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                                 z_step_kind=\"tron\")\n",
    "                clusters[cluster_number].fit(cluster_data_X[cluster_number])\n",
    "            fitted_ = np.dot(clusters[cluster_number].factors_,clusters[cluster_number].loadings_)+clusters[cluster_number].intercept_\n",
    "            cluster_predicted[cluster_number] = transformer.inverse_transform( clusters[cluster_number].predict(fitted_,n_ahead))\n",
    "            \n",
    "            # Save prediction\n",
    "            cluster_predicted[cluster_number] = pd.DataFrame(cluster_predicted[cluster_number]).transpose()\n",
    "            \n",
    "           # print(cluster_predicted[cluster_number].columns )\n",
    "           # print( cluster_data_true_values[cluster_number].columns)\n",
    "            cluster_predicted[cluster_number].columns  = cluster_data_true_values[cluster_number].columns \n",
    "            cluster_predicted[cluster_number].index = cluster_data_true_values[cluster_number].index\n",
    "            #save cluster score\n",
    "            cluster_error[cluster_number] =  score_function(cluster_data_true_values[cluster_number].values.ravel(),cluster_predicted[cluster_number].values.ravel())\n",
    "        print(\"Error per cluster: \",cluster_error)\n",
    "        # flatten true values\n",
    "        all_true_values  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_data_true_values.values())  ]])   )\n",
    "        # flatten prediction \n",
    "        all_predictions  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_predicted.values())  ]])   )\n",
    "        total_error =  score_function(np.array(all_true_values),np.array(all_predictions))\n",
    "        print(\"New error: \",round(total_error,4))\n",
    "        # If new score is worse - load last cluster and run final prediction. \n",
    "        if (total_error > previous_total_error):\n",
    "            if counter_iterations ==3:\n",
    "               # train_targets[cluster_column] = list(old_cluster_for_user.values())\n",
    "                print('Error has increased. Backtrack to previous iteration')\n",
    "                return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i-counter_iterations\n",
    "            counter_iterations+=1\n",
    "        else:\n",
    "            counter_iterations = 0 \n",
    "            previous_total_error = total_error\n",
    "            old_clusters = clusters.copy()\n",
    "            old_cluster_for_user = cluster_for_user.copy()\n",
    "            # Save all stuff\n",
    "        if (i>=limit):# return prev iteration\n",
    "            return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i\n",
    "        # Calculate score per user\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():\n",
    "                score_for_user[index] = score_function(row, cluster_predicted[cluster_number].loc[index,:])\n",
    "        # Assume that we haven't change any cluster\n",
    "        for x in cluster_has_changed:\n",
    "            cluster_has_changed[x]=False        \n",
    "        cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "       # old_clusters = {}\n",
    "        #old_cluster_for_user = {}\n",
    "        ii =0\n",
    "        reassignment_count = 0\n",
    "        if (i==0):\n",
    "            old_clusters = clusters.copy()\n",
    "        #Take observation x from cluster a, where error_x > error_a\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            # All ids in this cluster are already in cluster with lowest possible mae\n",
    "            if (cluster_number == cluster_lowest_mae):\n",
    "                continue\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():    \n",
    "            # User is in current cluster -> we can reassign him\n",
    "                if (cluster_number  == cluster_for_user[index]):\n",
    "                    if (score_for_user[index] > cluster_error[cluster_number]):\n",
    "                        ii+=1#counter\n",
    "                        # Have to refit changed clusters             \n",
    "                        cluster_has_changed[cluster_number] = True\n",
    "                        cluster_has_changed[cluster_lowest_mae] = True\n",
    "                        # Move observation x to the cluster with the lowest training erro\n",
    "                        train_targets.loc[index,cluster_column] = cluster_lowest_mae\n",
    "                        reassignment_count+=1\n",
    "                        if reassignment_count==recalculate_assignment:\n",
    "                            reassignment_count=0\n",
    "                            # RECALCULATION - only best performing cluster (because)- TODO:save snapshot of prev cluster for backtracking\n",
    "                            cluster_data_X_reassigned =  train_targets.loc[train_targets[cluster_column] == cluster_lowest_mae].drop(columns =columns_wo_TS)\n",
    "                            \n",
    "                            # print(  train_targets.loc[train_targets.cluster==2].drop(columns = ['cluster','V1']).shape)\n",
    "                            cluster_data_true_values_reassigned= test_targets.loc[ cluster_data_X_reassigned.index]\n",
    "                            cluster_data_X_reassigned =  transformer.fit_transform(cluster_data_X_reassigned.transpose())\n",
    "                            clusters_reassignemnt = clone(clusters[cluster_lowest_mae])\n",
    "                            clusters_reassignemnt.fit(cluster_data_X_reassigned)\n",
    "                            predd =  transformer.inverse_transform(clusters_reassignemnt.predict(cluster_data_X_reassigned ,n_ahead=n_ahead))\n",
    "                            cluster_predicted_reassigned = predd\n",
    "                            # Save prediction\n",
    "                            cluster_predicted_reassigned = pd.DataFrame(cluster_predicted_reassigned).transpose()\n",
    "                            cluster_error[cluster_lowest_mae] = score_function(cluster_data_true_values_reassigned.values.ravel(),cluster_predicted_reassigned.values.ravel())\n",
    "                            cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "        if (i==0):\n",
    "            old_cluster_for_user = cluster_for_user.copy() # as well as save last assigned clusters                  \n",
    "        print('Moved ', ii, ' users to another cluster \\n')   \n",
    "   # return old_clusters,old_cluster_for_user,cluster_parameters_for_gridsearch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prediction_after_clusterwise_reg(\n",
    "    X,Y,\n",
    "        dataset,\n",
    "    method,\n",
    "    frequency,\n",
    "    model,           \n",
    "                           step = 'unstated',\n",
    "                           clusters = None,\n",
    "                           clusters_parameters = None,\n",
    "                           new_clusters_for_users=None,\n",
    "                           cluster_column ='supercluster',\n",
    "                           scores_per_cluster = True,\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                           gridsearch_use = False):\n",
    "    \"\"\"\n",
    "    Performs final prediction and generates DataFrame for Alibaba Data\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    step : 'before_classifier' or 'after_classifier', for report generation\n",
    "    clusters: dictionary with models for each cluster. If None, then refit\n",
    "    clusters_parameters: dictionary with parameters for each cluster model\n",
    "    new_clusters_for_users: dictionary with clusters for each user. \n",
    "    cluster_column : column with given clusters\n",
    "    \n",
    "    scores_per_cluster : if False the score will be given for the all dataset, not by cluster\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    gridsearch_use : Run gridSeach (True) or use clusters_parameters (False)?\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    result : DataFrame with columns\n",
    "    cluster, user id, true target, prediction, rmse, model, method (all data, clustered), step (all data, before classifier, after classifier)\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()\n",
    "    n_ahead = test_targets.shape[1]\n",
    "    if (new_clusters_for_users is not None):\n",
    "        # we assume that the order in DataFrame is the same as in clusterwise regression\n",
    "        train_targets[cluster_column]=list(cluster_per_user.values()) \n",
    "    cluster_data_X = {}\n",
    "    cluster_data_true_values = {}\n",
    "    if (clusters_parameters == None or gridsearch_use==True):\n",
    "        clusters_parameters = {}\n",
    "    if (clusters == None or clusters_parameters ==None):\n",
    "        clusters = {}\n",
    "    for cluster_number in train_targets[cluster_column].unique():\n",
    "        cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "        cluster_data_true_values[cluster_number] = test_targets.iloc[ cluster_data_X[cluster_number].index]\n",
    "        cluster_data_true_values[cluster_number].index =   test_targets.iloc[ cluster_data_X[cluster_number].index].index\n",
    "        # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "        transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "        \n",
    "        cluster_data_X[cluster_number] = cluster_data_X[cluster_number].transpose()\n",
    "        cluster_data_X[cluster_number] = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "        if (cluster_number not in clusters_parameters or gridsearch_use==True):\n",
    "            clusters_parameters[cluster_number] = gridsearch_trmf(cluster_data_X[cluster_number],cluster_data_true_values[cluster_number])\n",
    "                    \n",
    "        if (cluster_number not in clusters or ( cluster_number not in  clusters_parameters) or gridsearch_use==True):\n",
    "            clusters[cluster_number ]= TRMFRegressor(**clusters_parameters[cluster_number], eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                             fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                             z_step_kind=\"tron\")\n",
    "                # If you feed clusters from clusterwise, you don't need to refit them\n",
    "            clusters[cluster_number ].fit(cluster_data_X[cluster_number])\n",
    "        fitted_ = np.dot(clusters[cluster_number].factors_,clusters[cluster_number].loadings_)+clusters[cluster_number].intercept_\n",
    "        predicted = transformer.inverse_transform( clusters[cluster_number].predict(fitted_,n_ahead))\n",
    "\n",
    "            \n",
    "        df_prediction = pd.DataFrame(predicted).transpose()\n",
    "        df_prediction.columns  = cluster_data_true_values[cluster_number].columns \n",
    "        df_prediction.index = cluster_data_true_values[cluster_number].index # here we need to set old cluster\n",
    "    \n",
    "        # Transform prediction for output\n",
    "        #MODEL\n",
    "        df_prediction.insert(0,'Score',score_function(cluster_data_true_values[cluster_number].values.ravel(),df_prediction.values.ravel(),training_series=fitted_))\n",
    "        df_prediction.insert(0,'model',model)\n",
    "        #Dataset\n",
    "        df_prediction.insert(0,'dataset',dataset)\n",
    "        #METHOD\n",
    "        df_prediction.insert(0,'method',method)\n",
    "        #STEP\n",
    "        df_prediction.insert(0,'frequency',frequency)\n",
    "        df_prediction.insert(0,'horizon','full')\n",
    "        df_prediction.insert(0,'cluster_id',str(cluster_column)+'_'+str(cluster_number))\n",
    "        #Wide to logn\n",
    "        df_prediction = df_prediction.reset_index().melt(id_vars =['Unnamed: 1','cluster_id','model','horizon','dataset','method','frequency','Score']).rename(index = str, columns={'variable':'date','value':'predicted_value','Unnamed: 1':'id'})\n",
    "        true_values=cluster_data_true_values[cluster_number].reset_index().melt(id_vars=['Unnamed: 1']).rename(index = str, columns={'variable':'date','value':'true_value','Unnamed: 1':'id'})\n",
    "        result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))\n",
    "    if scores_per_cluster == False:\n",
    "           result.Score = score_function(result.true_value,result.predicted_value,\n",
    "                             training_series=train_targets.drop(columns =columns_wo_TS).values.ravel())\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if it can work with Gridsearch CV(It should but, you can just pass None)\n",
    "class RegressorEnsemble(BaseEstimator):\n",
    "    def __init__(self, rgr, n_estimators=10):\n",
    "        self.rgr = rgr\n",
    "        self.n_estimators = n_estimators\n",
    "        self.rgrs = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.rgrs.append(clone(self.rgr))\n",
    "\n",
    "    def fit(self, X, y,\n",
    "              seed=None,):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        sd = np.random.seed()\n",
    "        print(sd)\n",
    "        for i in range(self.n_estimators):\n",
    "            self.rgrs[i].fit(X, y,random_seed = sd)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ans = self.rgrs[0].predict(X)\n",
    "        for i in range(1,self.n_estimators):\n",
    "            ans += self.rgrs[i].predict(X)\n",
    "        return ans / len(self.rgrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,max_depth=3,\n",
    "                                                          random_state=0))\n",
    "regr_multirf = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',\n",
    "                                                     n_estimators=50)) #tree_method='gpu_hist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error per cluster:  {1: 1.655143293378799, 2: 1.7406956276444272, 5: 1.6956489267182913, 3: 1.5392566880983616, 4: 1.6822268625650296, 6: 1.6630462243516302, 0: 1.583777351492016}\n",
      "New error:  1.6565\n",
      "Moved  500  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.0344274390689168, 2: 1.2879851643512459, 5: 1.058037744880844, 3: 1.8796138836006555, 4: 1.1130547791236602, 6: 1.1192378024890657, 0: 1.0022843691531855}\n",
      "New error:  1.6538\n",
      "Moved  563  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.7802264546030984, 2: 0.9253668941161803, 5: 0.8093429039916029, 3: 1.5502803486558048, 4: 0.9057622691565554, 6: 0.7750852323252116, 0: 1.8280205251929875}\n",
      "New error:  1.6643\n",
      "Moved  592  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.5952810997332433, 2: 0.7446109094576104, 5: 0.6267769135407741, 3: 0.9522093853939413, 4: 0.8086805384665287, 6: 1.8343630618563296, 0: 1.3047581545444658}\n",
      "New error:  1.6487\n",
      "Moved  607  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.828417290979273, 2: 0.5865451908178756, 5: 0.44315829775757865, 3: 0.5766710905754578, 4: 0.5407860742868362, 6: 1.2983151135043502, 0: 1.0612509390634066}\n",
      "New error:  1.6499\n",
      "Moved  593  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.3049105838757844, 2: 0.5390010378260317, 5: 1.8719401383895993, 3: 0.43331204244299937, 4: 0.38069335498085743, 6: 0.8877512161510851, 0: 0.8104923533487568}\n",
      "New error:  1.6523\n",
      "Moved  617  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.7913699721232917, 2: 0.5390010378260317, 5: 1.4742639517962683, 3: 0.3008917921845442, 4: 1.809242342056197, 6: 0.6643120626281739, 0: 0.6336021540206801}\n",
      "New error:  1.6487\n",
      "Moved  589  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.6141915703893528, 2: 0.5390010378260317, 5: 0.9244636683987695, 3: 1.8748463918075806, 4: 1.2807347515443468, 6: 0.6541663667036917, 0: 0.560649087434363}\n",
      "New error:  1.6508\n",
      "Moved  604  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.396024904594002, 2: 1.8312598934808426, 5: 0.5917183657071152, 3: 1.4874413974047893, 4: 0.9340237182694942, 6: 0.40946848100626654, 0: 0.5380426232227327}\n",
      "New error:  1.6505\n",
      "Moved  614  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.827321939627703, 2: 1.3413707738437615, 5: 0.4576649700075318, 3: 0.8410714659513547, 4: 0.6272950656716477, 6: 0.25, 0: 0.5151265975698653}\n",
      "New error:  1.6484\n",
      "Moved  609  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.2871521304698401, 2: 1.0299074746083396, 5: 0.25, 3: 0.7298404208388916, 4: 0.814024754720177, 6: 1.8348232263100006, 0: 0.5151265975698653}\n",
      "New error:  1.6505\n",
      "Moved  599  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.9024524886041525, 2: 0.6835312431002952, 5: 1.865073845724422, 3: 0.39165669381305585, 4: 0.47517014163781857, 6: 1.3222015140304713, 0: 0.5151265975698653}\n",
      "New error:  1.6523\n",
      "Moved  624  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.577641079144484, 2: 0.5568929524219558, 5: 1.4569705927374177, 3: 1.8010309973859917, 4: 0.3601426924109829, 6: 0.7749690443975099, 0: 0.5151265975698653}\n",
      "New error:  1.6487\n",
      "Moved  595  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.6179147069267394, 2: 0.3945005594139669, 5: 1.0427902882188376, 3: 1.2458519570471622, 4: 1.860504015932224, 6: 0.40927953652558857, 0: 0.5151265975698653}\n",
      "New error:  1.6488\n",
      "Error has increased. Backtrack to previous iteration\n",
      "CPU times: user 1min 10s, sys: 4min 23s, total: 5min 33s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FOR TRMF\n",
    "freq = 'future_flag_0'\n",
    "col = 'random initialization2'\n",
    "X = train_df\n",
    "Y  = test_df\n",
    "clusters, cluster_per_user,parameters,reassignment_iterations = clusterwise_regression(X,#.drop(columns =['V'+str(x) for x in list(range(2,817))]),\n",
    "                                                                    Y.drop(columns = ['Unnamed: 1']),\n",
    "                                                                    cluster_column= col,\n",
    "                columns_wo_TS=['Unnamed: 1','GMM_Pair','GMM_Voting','random initialization1','random initialization2'],\n",
    "                                                                        score_function=smape,\n",
    "                                                                       recalculate_assignment=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_trmf = final_prediction_after_clusterwise_reg(X,\n",
    "                                    Y.set_index(['Unnamed: 1']),\n",
    "                                    step ='before_classifier',\n",
    "                                    clusters = clusters,\n",
    "                                    clusters_parameters = parameters,\n",
    "                                    new_clusters_for_users = cluster_per_user,\n",
    "                                    gridsearch_use = None,\n",
    "                                    cluster_column=col,\n",
    "                                    scores_per_cluster = False , #important !False,\n",
    "                                    score_function=smape,#smape,#pinball_loss,#  (lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "                                    columns_wo_TS=['Unnamed: 1','GMM_Pair','GMM_Voting','random initialization1','random initialization2'],\n",
    "                                    \n",
    "                                    dataset = 'Huawei',\n",
    "                                    model = 'TRMF',\n",
    "                                    method = 'clusterwise',\n",
    "                                    frequency = freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_trmf['clusterwise_loss'] = 'SMAPE'\n",
    "dff_trmf['final_loss'] = 'SMAPE'\n",
    "dff_trmf['reassignments_iterations'] = reassignment_iterations\n",
    "res = res.append(dff_trmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error per cluster:  {1: 1.7148025965740117, 2: 1.7510526493997172, 5: 1.730354251999149, 3: 1.6354576429519745, 4: 1.7405710417553748, 6: 1.7526378752191831, 0: 1.6233113503045031}\n",
      "New error:  1.7119\n",
      "Moved  494  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.2255791334524961, 2: 1.259721629337031, 5: 1.2217348154843444, 3: 1.1378892613955842, 4: 1.3307294868693147, 6: 1.3097996396594433, 0: 1.9018749309383731}\n",
      "New error:  1.7176\n",
      "Moved  549  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.9223999123564584, 2: 0.9059144655091345, 5: 0.9441557746745175, 3: 1.8475539262203169, 4: 1.0065735945742702, 6: 1.010761453475965, 0: 1.6526415413169664}\n",
      "New error:  1.7085\n",
      "Moved  587  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.889937850636588, 2: 1.8735176945168575, 5: 0.7908028024041374, 3: 1.4989893725060004, 4: 0.8021920921931126, 6: 0.9012400449949423, 0: 1.1943529589828135}\n",
      "New error:  1.7158\n",
      "Moved  581  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.8018835394535708, 2: 1.5420080501988211, 5: 1.8608794445737946, 3: 1.1244478541556806, 4: 0.8103726226113919, 6: 0.8213309289501638, 0: 1.0192366451987191}\n",
      "New error:  1.7052\n",
      "Moved  590  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.8743174292295175, 2: 1.0737303572299428, 5: 1.477933314045124, 3: 0.9108460141951056, 4: 0.7205083746986274, 6: 0.7481677974118559, 0: 0.9096240084616235}\n",
      "New error:  1.7039\n",
      "Moved  588  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.530684583600089, 2: 0.8763678773368355, 5: 1.1559429211200634, 3: 0.9026865046997559, 4: 1.8650117869836365, 6: 0.8105903202584618, 0: 0.7718116627201954}\n",
      "New error:  1.7043\n",
      "Moved  587  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.2374533356684618, 2: 0.8539983044810184, 5: 0.8200192002377309, 3: 0.7010270926062115, 4: 1.5106448658099587, 6: 0.8556216249581745, 0: 1.881364474356641}\n",
      "New error:  1.7167\n",
      "Moved  587  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.9840564631588976, 2: 0.849973446047503, 5: 0.7909986278899076, 3: 1.8840102485479249, 4: 1.1567942366057586, 6: 0.8556216249581745, 0: 1.5411668829711171}\n",
      "New error:  1.7206\n",
      "Moved  597  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.8492148224646018, 2: 0.6603803976910468, 5: 1.8601866544236119, 3: 1.459598726073825, 4: 1.0017281189350047, 6: 0.8556216249581745, 0: 1.075869489601959}\n",
      "New error:  1.7014\n",
      "Moved  574  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 0.6795207588060963, 2: 1.868842714484363, 5: 1.5867272694081427, 3: 1.067624702434458, 4: 0.8906105510919102, 6: 0.8556216249581745, 0: 0.8668987048053018}\n",
      "New error:  1.7117\n",
      "Moved  577  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.8844057938758252, 2: 1.5261900844413159, 5: 1.1577554810966157, 3: 0.8972138303856149, 4: 0.6614033181199225, 6: 0.8556216249581745, 0: 0.8687167276993082}\n",
      "New error:  1.7116\n",
      "Moved  584  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.5725449732507242, 2: 1.0731607856817886, 5: 0.920206389757308, 3: 0.737483453595485, 4: 1.8758091524162035, 6: 0.8556216249581745, 0: 0.7403972877100582}\n",
      "New error:  1.7081\n",
      "Moved  575  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 1.1659586472146188, 2: 0.960502436544239, 5: 0.8286985954049894, 3: 1.8693976549051605, 4: 1.5685858765685226, 6: 0.8556216249581745, 0: 0.7252631992847827}\n",
      "New error:  1.7199\n",
      "Error has increased. Backtrack to previous iteration\n",
      "CPU times: user 1min 2s, sys: 4.16 s, total: 1min 6s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FOR SKLEARN\n",
    "X = train_df\n",
    "Y  = test_df\n",
    "modl = 'XGBoost'\n",
    "#regr = Ridge()\n",
    "regr = RegressorEnsemble(XGBRegressor(objective='reg:squarederror',n_estimators=125),1)\n",
    "col = 'random initialization2'\n",
    "freq = 'future_flag_0'\n",
    "clusters, cluster_per_user,parameters,reassignment_iterations = clusterwise_regression_sklearn(X,#.drop(columns =['V'+str(x) for x in list(range(2,817))]),\n",
    "                                                                    Y.drop(columns = ['Unnamed: 1']),\n",
    "                                                                    cluster_column=col,\n",
    "                                                                        columns_wo_TS=['Unnamed: 1','GMM_Pair','GMM_Voting','random initialization1','random initialization2'],\n",
    "                                                                        score_function=smape,\n",
    "                                                                       recalculate_assignment=1000,\n",
    "                                                                      horizon = True,\n",
    "                                                                       regr =regr,\n",
    "                                                                      regr_gridsearch_params = None\n",
    "                                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_sklearn = final_prediction_after_clusterwise_reg_sklearn(X,\n",
    "                                    Y.set_index(['Unnamed: 1']),\n",
    "                                    regr = regr,\n",
    "                                    clusters = clusters,\n",
    "                                    clusters_parameters = parameters,\n",
    "                                    new_clusters_for_users = cluster_per_user,\n",
    "                                    gridsearch_use = None,\n",
    "                                    cluster_column=col,\n",
    "                                    horizon = True, \n",
    "                                                            \n",
    "\n",
    "                                    scores_per_cluster = False , #important !False,\n",
    "                                    score_function=smape,#smape,#pinball_loss,#  (lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "                                    columns_wo_TS=['Unnamed: 1','GMM_Pair','GMM_Voting','random initialization1','random initialization2'],\n",
    "\n",
    "                                    dataset = 'Huawei',\n",
    "                                    model = modl,\n",
    "                                    method = 'clusterwise',\n",
    "                                    frequency = freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_sklearn['clusterwise_loss'] = 'SMAPE'\n",
    "dff_sklearn['final_loss'] = 'SMAPE'\n",
    "dff_sklearn['reassignments_iterations']  =reassignment_iterations\n",
    "res = res.append(dff_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def write_df(df: pd.DataFrame,\n",
    "             path: str,\n",
    "             project: str,\n",
    "             step: str,\n",
    "             format: str = 'csv') -> None:\n",
    "\n",
    "\n",
    "    title = path + time.strftime(\"%Y%m%d_%H%M\") + '_' + project + '__' + step + '.' + format\n",
    "\n",
    "    if format == 'csv':\n",
    "        df.to_csv(title)\n",
    "\n",
    "    if format == 'h5':\n",
    "        df.to_hdf(title, step, table=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(res, export,'huawei','1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  7, 12, 10])"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.read_csv(\"Export/ICDM19/20190605_2237_huawei__1.csv\").reassignments_iterations.\n",
    "#unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
