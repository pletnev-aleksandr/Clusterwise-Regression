{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_for_classification = 3000\n",
    "#prediction\n",
    "prediction_path = \"Export/cdnow/cdnow_prediction_trmf_variantB_\"+str(batch_size_for_classification+2000)+'_.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from trmf import TRMFRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import random\n",
    "import rpy2\n",
    "import math\n",
    "import trmf\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.packages as rpackages\n",
    "import rpy2.robjects as robjects\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1) # select the first mirror in the list\n",
    "packnames = ['forecast']\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "try:\n",
    "    base = importr('forecast')\n",
    "except:\n",
    "    utils.install_packages(StrVector('forecast'))\n",
    "    base = importr('forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path fo import files\n",
    "#input data\n",
    "import_data = \"import/CDNOW_master.txt\"\n",
    "\n",
    "#prediction\n",
    "prediction_path = \"Export/cdnow/cdnow_prediction_variantB.csv\"\n",
    "# clusters\n",
    "import_clusters = \"clusters/results_cdnow.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preprocessing alibaba + normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(import_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Date</th>\n",
       "      <th>1997-01-01 00:00:00</th>\n",
       "      <th>1997-01-02 00:00:00</th>\n",
       "      <th>1997-01-03 00:00:00</th>\n",
       "      <th>1997-01-04 00:00:00</th>\n",
       "      <th>1997-01-05 00:00:00</th>\n",
       "      <th>1997-01-06 00:00:00</th>\n",
       "      <th>1997-01-07 00:00:00</th>\n",
       "      <th>1997-01-08 00:00:00</th>\n",
       "      <th>1997-01-09 00:00:00</th>\n",
       "      <th>1997-01-10 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>1998-06-21 00:00:00</th>\n",
       "      <th>1998-06-22 00:00:00</th>\n",
       "      <th>1998-06-23 00:00:00</th>\n",
       "      <th>1998-06-24 00:00:00</th>\n",
       "      <th>1998-06-25 00:00:00</th>\n",
       "      <th>1998-06-26 00:00:00</th>\n",
       "      <th>1998-06-27 00:00:00</th>\n",
       "      <th>1998-06-28 00:00:00</th>\n",
       "      <th>1998-06-29 00:00:00</th>\n",
       "      <th>1998-06-30 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7892</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 546 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Date  1997-01-01  1997-01-02  1997-01-03  1997-01-04  1997-01-05  1997-01-06  \\\n",
       "id                                                                             \n",
       "680          NaN         NaN       15.96         NaN         NaN         NaN   \n",
       "7892         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "Date  1997-01-07  1997-01-08  1997-01-09  1997-01-10  ...  1998-06-21  \\\n",
       "id                                                    ...               \n",
       "680          NaN         NaN         NaN         NaN  ...         NaN   \n",
       "7892         NaN         NaN         NaN         NaN  ...         NaN   \n",
       "\n",
       "Date  1998-06-22  1998-06-23  1998-06-24  1998-06-25  1998-06-26  1998-06-27  \\\n",
       "id                                                                             \n",
       "680          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "7892         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "Date  1998-06-28  1998-06-29  1998-06-30  \n",
       "id                                        \n",
       "680          NaN         NaN         NaN  \n",
       "7892         NaN         NaN         NaN  \n",
       "\n",
       "[2 rows x 546 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(import_data,header=None,names  =['id','Date','disks','price_purchase'],delim_whitespace=True,)\n",
    "df['Date'] = df[\"Date\"].apply(lambda x:datetime.datetime.strptime(str(x),\"%Y%m%d\"))\n",
    "df = df.drop(columns = ['disks'])\n",
    "df = df.groupby(by=['id','Date']).sum().reset_index()\n",
    "df = df.pivot(columns = 'Date',index = 'id',values = 'price_purchase')\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Choose TS with given clusters and divide into test/train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clusters_r</th>\n",
       "      <th>clusters_m</th>\n",
       "      <th>clusters_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11770</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14675</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10026</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13997</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  clusters_r  clusters_m  clusters_f\n",
       "0       11770           0           0           0\n",
       "1       14675          17           0           8\n",
       "2       10026           2           0           3\n",
       "3       13997           1           0           2\n",
       "4         741           0           0           0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_TS  = pd.read_csv(import_clusters)\n",
    "cluster_TS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = cluster_TS.merge(df,left_on = \"Unnamed: 0\", right_on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters_r</th>\n",
       "      <th>clusters_m</th>\n",
       "      <th>clusters_f</th>\n",
       "      <th>1997-01-01 00:00:00</th>\n",
       "      <th>1997-01-02 00:00:00</th>\n",
       "      <th>1997-01-03 00:00:00</th>\n",
       "      <th>1997-01-04 00:00:00</th>\n",
       "      <th>1997-01-05 00:00:00</th>\n",
       "      <th>1997-01-06 00:00:00</th>\n",
       "      <th>1997-01-07 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>1998-06-21 00:00:00</th>\n",
       "      <th>1998-06-22 00:00:00</th>\n",
       "      <th>1998-06-23 00:00:00</th>\n",
       "      <th>1998-06-24 00:00:00</th>\n",
       "      <th>1998-06-25 00:00:00</th>\n",
       "      <th>1998-06-26 00:00:00</th>\n",
       "      <th>1998-06-27 00:00:00</th>\n",
       "      <th>1998-06-28 00:00:00</th>\n",
       "      <th>1998-06-29 00:00:00</th>\n",
       "      <th>1998-06-30 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11770</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14675</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10026</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       clusters_r  clusters_m  clusters_f  1997-01-01 00:00:00  \\\n",
       "id                                                               \n",
       "11770           0           0           0                  NaN   \n",
       "14675          17           0           8                  NaN   \n",
       "10026           2           0           3                  NaN   \n",
       "13997           1           0           2                  NaN   \n",
       "741             0           0           0                  NaN   \n",
       "\n",
       "       1997-01-02 00:00:00  1997-01-03 00:00:00  1997-01-04 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  NaN                  NaN                  NaN   \n",
       "14675                  NaN                  NaN                  NaN   \n",
       "10026                  NaN                  NaN                  NaN   \n",
       "13997                  NaN                  NaN                  NaN   \n",
       "741                    NaN                  NaN                43.13   \n",
       "\n",
       "       1997-01-05 00:00:00  1997-01-06 00:00:00  1997-01-07 00:00:00  ...  \\\n",
       "id                                                                    ...   \n",
       "11770                  NaN                  NaN                  NaN  ...   \n",
       "14675                  NaN                  NaN                  NaN  ...   \n",
       "10026                  NaN                  NaN                  NaN  ...   \n",
       "13997                  NaN                  NaN                  NaN  ...   \n",
       "741                    NaN                  NaN                  NaN  ...   \n",
       "\n",
       "       1998-06-21 00:00:00  1998-06-22 00:00:00  1998-06-23 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  NaN                  NaN                  NaN   \n",
       "14675                  NaN                  NaN                  NaN   \n",
       "10026                  NaN                  NaN                  NaN   \n",
       "13997                  NaN                  NaN                  NaN   \n",
       "741                    NaN                  NaN                  NaN   \n",
       "\n",
       "       1998-06-24 00:00:00  1998-06-25 00:00:00  1998-06-26 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  NaN                  NaN                  NaN   \n",
       "14675                  NaN                  NaN                  NaN   \n",
       "10026                  NaN                  NaN                  NaN   \n",
       "13997                  NaN                  NaN                  NaN   \n",
       "741                    NaN                  NaN                  NaN   \n",
       "\n",
       "       1998-06-27 00:00:00  1998-06-28 00:00:00  1998-06-29 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  NaN                  NaN                  NaN   \n",
       "14675                  NaN                  NaN                  NaN   \n",
       "10026                  NaN                  NaN                  NaN   \n",
       "13997                  NaN                  NaN                  NaN   \n",
       "741                    NaN                  NaN                  NaN   \n",
       "\n",
       "       1998-06-30 00:00:00  \n",
       "id                          \n",
       "11770                  NaN  \n",
       "14675                  NaN  \n",
       "10026                  NaN  \n",
       "13997                  NaN  \n",
       "741                    NaN  \n",
       "\n",
       "[5 rows x 549 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.rename(index = str, columns={\"Unnamed: 0\": \"id\"}).set_index('id')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Date</th>\n",
       "      <th>1997-01-01 00:00:00</th>\n",
       "      <th>1997-01-02 00:00:00</th>\n",
       "      <th>1997-01-03 00:00:00</th>\n",
       "      <th>1997-01-04 00:00:00</th>\n",
       "      <th>1997-01-05 00:00:00</th>\n",
       "      <th>1997-01-06 00:00:00</th>\n",
       "      <th>1997-01-07 00:00:00</th>\n",
       "      <th>1997-01-08 00:00:00</th>\n",
       "      <th>1997-01-09 00:00:00</th>\n",
       "      <th>1997-01-10 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>1998-06-21 00:00:00</th>\n",
       "      <th>1998-06-22 00:00:00</th>\n",
       "      <th>1998-06-23 00:00:00</th>\n",
       "      <th>1998-06-24 00:00:00</th>\n",
       "      <th>1998-06-25 00:00:00</th>\n",
       "      <th>1998-06-26 00:00:00</th>\n",
       "      <th>1998-06-27 00:00:00</th>\n",
       "      <th>1998-06-28 00:00:00</th>\n",
       "      <th>1998-06-29 00:00:00</th>\n",
       "      <th>1998-06-30 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10751</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17831</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23136</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 546 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Date   1997-01-01  1997-01-02  1997-01-03  1997-01-04  1997-01-05  1997-01-06  \\\n",
       "id                                                                              \n",
       "10063         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "10751         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "17831         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "23136         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "5908          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "Date   1997-01-07  1997-01-08  1997-01-09  1997-01-10  ...  1998-06-21  \\\n",
       "id                                                     ...               \n",
       "10063         NaN         NaN         NaN         NaN  ...         NaN   \n",
       "10751         NaN         NaN         NaN         NaN  ...         NaN   \n",
       "17831         NaN         NaN         NaN         NaN  ...         NaN   \n",
       "23136         NaN         NaN         NaN         NaN  ...         NaN   \n",
       "5908          NaN         NaN         NaN         NaN  ...         NaN   \n",
       "\n",
       "Date   1998-06-22  1998-06-23  1998-06-24  1998-06-25  1998-06-26  1998-06-27  \\\n",
       "id                                                                              \n",
       "10063         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "10751         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "17831         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "23136         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "5908          NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "Date   1998-06-28  1998-06-29  1998-06-30  \n",
       "id                                         \n",
       "10063         NaN         NaN         NaN  \n",
       "10751         NaN         NaN         NaN  \n",
       "17831         NaN         NaN         NaN  \n",
       "23136         NaN         NaN         NaN  \n",
       "5908          NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 546 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for this we will predict clusters\n",
    "df_test= df.drop(df_train.index.values,axis=0).sample(batch_size_for_classification)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAN -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters_r</th>\n",
       "      <th>clusters_m</th>\n",
       "      <th>clusters_f</th>\n",
       "      <th>1997-01-01 00:00:00</th>\n",
       "      <th>1997-01-02 00:00:00</th>\n",
       "      <th>1997-01-03 00:00:00</th>\n",
       "      <th>1997-01-04 00:00:00</th>\n",
       "      <th>1997-01-05 00:00:00</th>\n",
       "      <th>1997-01-06 00:00:00</th>\n",
       "      <th>1997-01-07 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>1998-06-21 00:00:00</th>\n",
       "      <th>1998-06-22 00:00:00</th>\n",
       "      <th>1998-06-23 00:00:00</th>\n",
       "      <th>1998-06-24 00:00:00</th>\n",
       "      <th>1998-06-25 00:00:00</th>\n",
       "      <th>1998-06-26 00:00:00</th>\n",
       "      <th>1998-06-27 00:00:00</th>\n",
       "      <th>1998-06-28 00:00:00</th>\n",
       "      <th>1998-06-29 00:00:00</th>\n",
       "      <th>1998-06-30 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15492</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7468</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       clusters_r  clusters_m  clusters_f  1997-01-01 00:00:00  \\\n",
       "id                                                               \n",
       "15492           6           0           9                  0.0   \n",
       "7468            0           0           6                  0.0   \n",
       "\n",
       "       1997-01-02 00:00:00  1997-01-03 00:00:00  1997-01-04 00:00:00  \\\n",
       "id                                                                     \n",
       "15492                  0.0                  0.0                  0.0   \n",
       "7468                   0.0                  0.0                  0.0   \n",
       "\n",
       "       1997-01-05 00:00:00  1997-01-06 00:00:00  1997-01-07 00:00:00  ...  \\\n",
       "id                                                                    ...   \n",
       "15492                  0.0                  0.0                  0.0  ...   \n",
       "7468                   0.0                  0.0                  0.0  ...   \n",
       "\n",
       "       1998-06-21 00:00:00  1998-06-22 00:00:00  1998-06-23 00:00:00  \\\n",
       "id                                                                     \n",
       "15492                  0.0                  0.0                  0.0   \n",
       "7468                   0.0                  0.0                  0.0   \n",
       "\n",
       "       1998-06-24 00:00:00  1998-06-25 00:00:00  1998-06-26 00:00:00  \\\n",
       "id                                                                     \n",
       "15492                  0.0                  0.0                  0.0   \n",
       "7468                   0.0                  0.0                  0.0   \n",
       "\n",
       "       1998-06-27 00:00:00  1998-06-28 00:00:00  1998-06-29 00:00:00  \\\n",
       "id                                                                     \n",
       "15492                  0.0                  0.0                  0.0   \n",
       "7468                   0.0                  0.0                  0.0   \n",
       "\n",
       "       1998-06-30 00:00:00  \n",
       "id                          \n",
       "15492                  0.0  \n",
       "7468                   0.0  \n",
       "\n",
       "[2 rows x 549 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)\n",
    "df_train.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output File:\n",
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prediction with given clusters on train data (2000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets, test_targets= train_test_split(df_train.transpose(), test_size=.127, shuffle=False)\n",
    "train_targets = train_targets.transpose()\n",
    "test_targets = test_targets.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1 GridSearch function for TMRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = TRMFRegressor(n_components=1,\n",
    "                 n_order=0,\n",
    "                 fit_regression=False,\n",
    "                 fit_intercept=True,\n",
    "                 nonnegative_factors=True,\n",
    "                 n_max_mf_iter=5)\n",
    "\n",
    "\n",
    "def gridsearch_trmf(X,test_targets):\n",
    "    grid = ParameterGrid(dict(\n",
    "    n_components=np.r_[1:2],\n",
    "    n_order=np.r_[1:2],\n",
    "    C_Z=np.logspace(-2, 1,num=1),\n",
    "    C_F=np.logspace(-2, 1,num=1),\n",
    "    C_phi=np.logspace(-2, 1, num=1),\n",
    "    eta_Z=np.linspace(0.05, 0.95,num=1),\n",
    "    ))\n",
    "    \n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = transformer.fit_transform(X)\n",
    "\n",
    "    \n",
    "    # run the experiment in parallel\n",
    "    par_ = Parallel(n_jobs=-1, verbose=0)\n",
    "    results = par_(delayed(helper)(par, X, test_targets.transpose(),transformer) for par in grid)\n",
    "    \n",
    "    keys = ['n_order', 'n_components', 'eta_Z', 'C_phi', 'C_Z', 'C_F']\n",
    "    data = dict((tuple(par[k] for k in keys), rmse,) for par, rmse in results)\n",
    "    sr = pd.Series(data, name=\"rmse\").sort_index().rename_axis(keys)\n",
    "    cube = sr.values.reshape(*[len(grid.param_grid[0][k]) for k in keys])\n",
    "    stepping = [grid.param_grid[0][k] for k in keys]\n",
    "    # find the flat index of the smallest value\n",
    "    flat_index = np.argmin(cube)\n",
    "    # ... and unravel into into a multidimensional index\n",
    "    index = np.unravel_index(flat_index, cube.shape)\n",
    "    # collect the best paramaters from the grid\n",
    "    best_ = {k: grid.param_grid[0][k][i] for k, i in zip(keys, index)}\n",
    "    return best_\n",
    "\n",
    "transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "\n",
    "def helper(par, train, test,  transformer,base=base):\n",
    "    basee = TRMFRegressor(n_components=1,\n",
    "                     n_order=0,\n",
    "                     fit_regression=False,\n",
    "                     fit_intercept=True,\n",
    "                     nonnegative_factors=True,\n",
    "                     n_max_mf_iter=5)\n",
    "    \n",
    "    # clone, set parameters and fit\n",
    "  #  print(train.shape)\n",
    "    #print(test.shape)\n",
    "    trmf = clone(basee).set_params(**par).fit(train)\n",
    "    \n",
    "    # predict and return\n",
    "    pred = transformer.inverse_transform(\n",
    "        trmf.predict(n_ahead=len(test)))\n",
    "\n",
    "    return par, mean_squared_error(test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 TMRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead = test_targets.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters_r</th>\n",
       "      <th>clusters_m</th>\n",
       "      <th>clusters_f</th>\n",
       "      <th>1997-01-01 00:00:00</th>\n",
       "      <th>1997-01-02 00:00:00</th>\n",
       "      <th>1997-01-03 00:00:00</th>\n",
       "      <th>1997-01-04 00:00:00</th>\n",
       "      <th>1997-01-05 00:00:00</th>\n",
       "      <th>1997-01-06 00:00:00</th>\n",
       "      <th>1997-01-07 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>1998-04-12 00:00:00</th>\n",
       "      <th>1998-04-13 00:00:00</th>\n",
       "      <th>1998-04-14 00:00:00</th>\n",
       "      <th>1998-04-15 00:00:00</th>\n",
       "      <th>1998-04-16 00:00:00</th>\n",
       "      <th>1998-04-17 00:00:00</th>\n",
       "      <th>1998-04-18 00:00:00</th>\n",
       "      <th>1998-04-19 00:00:00</th>\n",
       "      <th>1998-04-20 00:00:00</th>\n",
       "      <th>1998-04-21 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11770</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14675</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10026</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 479 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       clusters_r  clusters_m  clusters_f  1997-01-01 00:00:00  \\\n",
       "id                                                               \n",
       "11770         0.0         0.0         0.0                  0.0   \n",
       "14675        17.0         0.0         8.0                  0.0   \n",
       "10026         2.0         0.0         3.0                  0.0   \n",
       "13997         1.0         0.0         2.0                  0.0   \n",
       "741           0.0         0.0         0.0                  0.0   \n",
       "\n",
       "       1997-01-02 00:00:00  1997-01-03 00:00:00  1997-01-04 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  0.0                  0.0                 0.00   \n",
       "14675                  0.0                  0.0                 0.00   \n",
       "10026                  0.0                  0.0                 0.00   \n",
       "13997                  0.0                  0.0                 0.00   \n",
       "741                    0.0                  0.0                43.13   \n",
       "\n",
       "       1997-01-05 00:00:00  1997-01-06 00:00:00  1997-01-07 00:00:00  ...  \\\n",
       "id                                                                    ...   \n",
       "11770                  0.0                  0.0                  0.0  ...   \n",
       "14675                  0.0                  0.0                  0.0  ...   \n",
       "10026                  0.0                  0.0                  0.0  ...   \n",
       "13997                  0.0                  0.0                  0.0  ...   \n",
       "741                    0.0                  0.0                  0.0  ...   \n",
       "\n",
       "       1998-04-12 00:00:00  1998-04-13 00:00:00  1998-04-14 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  0.0                  0.0                  0.0   \n",
       "14675                  0.0                  0.0                  0.0   \n",
       "10026                  0.0                  0.0                  0.0   \n",
       "13997                  0.0                  0.0                  0.0   \n",
       "741                    0.0                  0.0                  0.0   \n",
       "\n",
       "       1998-04-15 00:00:00  1998-04-16 00:00:00  1998-04-17 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  0.0                  0.0                  0.0   \n",
       "14675                  0.0                  0.0                  0.0   \n",
       "10026                  0.0                  0.0                  0.0   \n",
       "13997                  0.0                  0.0                  0.0   \n",
       "741                    0.0                  0.0                  0.0   \n",
       "\n",
       "       1998-04-18 00:00:00  1998-04-19 00:00:00  1998-04-20 00:00:00  \\\n",
       "id                                                                     \n",
       "11770                  0.0                  0.0                  0.0   \n",
       "14675                  0.0                  0.0                  0.0   \n",
       "10026                  0.0                  0.0                  0.0   \n",
       "13997                  0.0                  0.0                  0.0   \n",
       "741                    0.0                  0.0                  0.0   \n",
       "\n",
       "       1998-04-21 00:00:00  \n",
       "id                          \n",
       "11770                  0.0  \n",
       "14675                  0.0  \n",
       "10026                  0.0  \n",
       "13997                  0.0  \n",
       "741                    0.0  \n",
       "\n",
       "[5 rows x 479 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 8min 39s, total: 9min 55s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create TRMF Models\n",
    "for cluster_number in df_train.clusters_r.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_r == cluster_number].drop(columns = ['clusters_m','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    \n",
    "    \n",
    "    # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = X.transpose()\n",
    "    X = transformer.fit_transform(X)\n",
    "    #GridSearchCV for TRMF\n",
    "    best = gridsearch_trmf(X,true_values)\n",
    "    #X = X.transpose()\n",
    "    \n",
    "    cluster = TRMFRegressor(**best, eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                     fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                     z_step_kind=\"tron\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster.fit(X)\n",
    "    #predicted = cluster.predict(X=X, n_ahead=n_ahead)\n",
    "    predicted = transformer.inverse_transform( cluster.predict(X=X, n_ahead=n_ahead))\n",
    "    X = X.transpose()\n",
    "    df_prediction = pd.DataFrame(predicted).transpose()\n",
    "    df_prediction.columns  = true_values.columns \n",
    "    df_prediction.index = true_values.index\n",
    "    \n",
    "    # Transform prediction for output\n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','trmf')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_R')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['id','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 1min 28s, total: 1min 42s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create TRMF Models\n",
    "for cluster_number in df_train.clusters_m.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_m == cluster_number].drop(columns = ['clusters_r','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    \n",
    "    \n",
    "    # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = X.transpose()\n",
    "    X = transformer.fit_transform(X)\n",
    "    #GridSearchCV for TRMF\n",
    "    best = gridsearch_trmf(X,true_values)\n",
    "    #X = X.transpose()\n",
    "    \n",
    "    cluster = TRMFRegressor(**best, eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                     fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                     z_step_kind=\"tron\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster.fit(X)\n",
    "    #predicted = cluster.predict(X=X, n_ahead=n_ahead)\n",
    "    predicted = transformer.inverse_transform( cluster.predict(X=X, n_ahead=n_ahead))\n",
    "    X = X.transpose()\n",
    "    df_prediction = pd.DataFrame(predicted).transpose()\n",
    "    df_prediction.columns  = true_values.columns \n",
    "    df_prediction.index = true_values.index\n",
    "    \n",
    "    # Transform prediction for output\n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','trmf')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_M')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['id','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 10min 52s, total: 12min 25s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create TRMF Models\n",
    "for cluster_number in df_train.clusters_f.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_f == cluster_number].drop(columns = ['clusters_r','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    \n",
    "    \n",
    "    # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = X.transpose()\n",
    "    X = transformer.fit_transform(X)\n",
    "    #GridSearchCV for TRMF\n",
    "    best = gridsearch_trmf(X,true_values)\n",
    "    #X = X.transpose()\n",
    "    \n",
    "    cluster = TRMFRegressor(**best, eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                     fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                     z_step_kind=\"tron\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster.fit(X)\n",
    "    #predicted = cluster.predict(X=X, n_ahead=n_ahead)\n",
    "    predicted = transformer.inverse_transform( cluster.predict(X=X, n_ahead=n_ahead))\n",
    "    X = X.transpose()\n",
    "    df_prediction = pd.DataFrame(predicted).transpose()\n",
    "    df_prediction.columns  = true_values.columns \n",
    "    df_prediction.index = true_values.index\n",
    "    \n",
    "    # Transform prediction for output\n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','trmf')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_F')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['id','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 100 ms, total: 21.2 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base = importr('forecast')\n",
    "# Create Theta Models\n",
    "for cluster_number in df_train.clusters_r.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_r == cluster_number].drop(columns = ['clusters_m','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    # Scaling\n",
    "    scl =MinMaxScaler() #StandardScaler(with_mean=True, with_std=True)\n",
    "    X = scl.fit_transform(X)\n",
    "    #THETA\n",
    "    df_prediction = pd.DataFrame(columns = test_targets.columns)\n",
    "\n",
    "    train_targets_scaled = pd.DataFrame(X)\n",
    "    train_targets_scaled.index = true_values.index\n",
    "\n",
    "\n",
    "    for index, row in train_targets_scaled.iterrows():\n",
    "        res = robjects.FloatVector(row.drop(columns = ['uid']))\n",
    "        r_prediction = base.thetaf(res,70)[1]\n",
    "        df_prediction.loc[index] = r_prediction\n",
    "    #Transform prediction for output \n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','theta')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_R')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['index','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value','index':'id'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 0 ns, total: 19.6 s\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Theta Models\n",
    "for cluster_number in df_train.clusters_m.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_m == cluster_number].drop(columns = ['clusters_r','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    # Scaling\n",
    "    scl =MinMaxScaler() #StandardScaler(with_mean=True, with_std=True)\n",
    "    X = scl.fit_transform(X)\n",
    "    #THETA\n",
    "    df_prediction = pd.DataFrame(columns = test_targets.columns)\n",
    "\n",
    "    train_targets_scaled = pd.DataFrame(X)\n",
    "    train_targets_scaled.index = true_values.index\n",
    "\n",
    "\n",
    "    for index, row in train_targets_scaled.iterrows():\n",
    "        res = robjects.FloatVector(row.drop(columns = ['uid']))\n",
    "        r_prediction = base.thetaf(res,70)[1]\n",
    "        df_prediction.loc[index] = r_prediction\n",
    "    #Transform prediction for output \n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','theta')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_M')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['index','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value','index':'id'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 32 ms, total: 20.8 s\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Theta Models\n",
    "for cluster_number in df_train.clusters_f.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_f == cluster_number].drop(columns = ['clusters_r','clusters_m'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    # Scaling\n",
    "    scl =MinMaxScaler() #StandardScaler(with_mean=True, with_std=True)\n",
    "    X = scl.fit_transform(X)\n",
    "    #THETA\n",
    "    df_prediction = pd.DataFrame(columns = test_targets.columns)\n",
    "\n",
    "    train_targets_scaled = pd.DataFrame(X)\n",
    "    train_targets_scaled.index = true_values.index\n",
    "\n",
    "\n",
    "    for index, row in train_targets_scaled.iterrows():\n",
    "        res = robjects.FloatVector(row.drop(columns = ['uid']))\n",
    "        r_prediction = base.thetaf(res,70)[1]\n",
    "        df_prediction.loc[index] = r_prediction\n",
    "    #Transform prediction for output \n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','theta')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_F')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['index','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value','index':'id'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training the classifier to predict cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the classifier on the same length as the length used for training (all minus 70)\n",
    "# Data for training Classifier\n",
    "X_2000_train,test_2000 = train_test_split(df_train.transpose(), test_size=.127, shuffle=False)\n",
    "X_2000_train = X_2000_train.transpose()\n",
    "test_2000 = test_2000.transpose()\n",
    "# Predict clusters using these data\n",
    "X_600_train, test_600= train_test_split(df_test.transpose(), test_size=.127, shuffle=False)\n",
    "X_600_train = X_600_train.transpose()\n",
    "\n",
    "\n",
    "test_600.transpose()\n",
    "\n",
    "y_train_2000_r = df_train.clusters_r\n",
    "y_train_2000_f = df_train.clusters_f\n",
    "y_train_2000_m = df_train.clusters_m\n",
    "\n",
    "X_2000_train = X_2000_train.drop(columns = ['clusters_r','clusters_m','clusters_f'])\n",
    "X_600_train.columns = X_2000_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier \n",
    "params = {\n",
    "        'min_child_weight': [1],# 5, 10],\n",
    "        'gamma': [0.5],#, 1, 2, 5],\n",
    "        'subsample': [0.6],# 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8],#, 1.0],\n",
    "        'max_depth': [3]\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:   15.4s remaining:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 928 ms, total: 1min 4s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBClassifier( nthread=6,  seed=0, silent=True)\n",
    "clf = GridSearchCV(clf,params,cv=3,n_jobs=-1,verbose=2)\n",
    "clf.fit(X_2000_train,y_train_2000_r)\n",
    "xgb = clf.best_estimator_\n",
    "xgb.fit(X_2000_train,y_train_2000_r)\n",
    "predicted_clusters = xgb.predict(X_600_train)\n",
    "\n",
    "df_test.insert(0,'clusters_r',predicted_clusters)\n",
    "#12 fits - 2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:    2.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 0 ns, total: 2.21 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBClassifier( nthread=6,  seed=0, silent=True)\n",
    "clf = GridSearchCV(clf,params,cv=3,n_jobs=-1,verbose=2)\n",
    "clf.fit(X_2000_train,y_train_2000_m)\n",
    "xgb = clf.best_estimator_\n",
    "xgb.fit(X_2000_train,y_train_2000_m)\n",
    "predicted_clusters = xgb.predict(X_600_train)\n",
    "\n",
    "df_test.insert(0,'clusters_m',predicted_clusters)\n",
    "# 12 fits - 9 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 2.3 Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:   13.3s remaining:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:   14.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.9 s, sys: 0 ns, total: 54.9 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = XGBClassifier( nthread=6,  seed=0, silent=True)\n",
    "clf = GridSearchCV(clf,params,cv=3,n_jobs=-1,verbose=2)\n",
    "clf.fit(X_2000_train,y_train_2000_f)\n",
    "xgb = clf.best_estimator_\n",
    "xgb.fit(X_2000_train,y_train_2000_f)\n",
    "predicted_clusters = xgb.predict(X_600_train)\n",
    "\n",
    "df_test.insert(0,'clusters_f',predicted_clusters)\n",
    "#12 fits - 90 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Date</th>\n",
       "      <th>clusters_f</th>\n",
       "      <th>clusters_m</th>\n",
       "      <th>clusters_r</th>\n",
       "      <th>1997-01-01 00:00:00</th>\n",
       "      <th>1997-01-02 00:00:00</th>\n",
       "      <th>1997-01-03 00:00:00</th>\n",
       "      <th>1997-01-04 00:00:00</th>\n",
       "      <th>1997-01-05 00:00:00</th>\n",
       "      <th>1997-01-06 00:00:00</th>\n",
       "      <th>1997-01-07 00:00:00</th>\n",
       "      <th>...</th>\n",
       "      <th>1998-06-21 00:00:00</th>\n",
       "      <th>1998-06-22 00:00:00</th>\n",
       "      <th>1998-06-23 00:00:00</th>\n",
       "      <th>1998-06-24 00:00:00</th>\n",
       "      <th>1998-06-25 00:00:00</th>\n",
       "      <th>1998-06-26 00:00:00</th>\n",
       "      <th>1998-06-27 00:00:00</th>\n",
       "      <th>1998-06-28 00:00:00</th>\n",
       "      <th>1998-06-29 00:00:00</th>\n",
       "      <th>1998-06-30 00:00:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11259</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7225</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16679</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23454</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Date   clusters_f  clusters_m  clusters_r  1997-01-01 00:00:00  \\\n",
       "id                                                               \n",
       "11259           0           0           0                  0.0   \n",
       "7225            0           0           0                  0.0   \n",
       "16679           0           0           0                  0.0   \n",
       "23454           9           0          15                  0.0   \n",
       "18204           0           0           0                  0.0   \n",
       "\n",
       "Date   1997-01-02 00:00:00  1997-01-03 00:00:00  1997-01-04 00:00:00  \\\n",
       "id                                                                     \n",
       "11259                  0.0                  0.0                  0.0   \n",
       "7225                   0.0                  0.0                  0.0   \n",
       "16679                  0.0                  0.0                  0.0   \n",
       "23454                  0.0                  0.0                  0.0   \n",
       "18204                  0.0                  0.0                  0.0   \n",
       "\n",
       "Date   1997-01-05 00:00:00  1997-01-06 00:00:00  1997-01-07 00:00:00  ...  \\\n",
       "id                                                                    ...   \n",
       "11259                  0.0                  0.0                  0.0  ...   \n",
       "7225                   0.0                  0.0                  0.0  ...   \n",
       "16679                  0.0                  0.0                  0.0  ...   \n",
       "23454                  0.0                  0.0                  0.0  ...   \n",
       "18204                  0.0                  0.0                  0.0  ...   \n",
       "\n",
       "Date   1998-06-21 00:00:00  1998-06-22 00:00:00  1998-06-23 00:00:00  \\\n",
       "id                                                                     \n",
       "11259                  0.0                  0.0                  0.0   \n",
       "7225                   0.0                  0.0                  0.0   \n",
       "16679                  0.0                  0.0                  0.0   \n",
       "23454                  0.0                  0.0                  0.0   \n",
       "18204                  0.0                  0.0                  0.0   \n",
       "\n",
       "Date   1998-06-24 00:00:00  1998-06-25 00:00:00  1998-06-26 00:00:00  \\\n",
       "id                                                                     \n",
       "11259                  0.0                  0.0                  0.0   \n",
       "7225                   0.0                  0.0                  0.0   \n",
       "16679                  0.0                  0.0                  0.0   \n",
       "23454                  0.0                  0.0                  0.0   \n",
       "18204                  0.0                  0.0                  0.0   \n",
       "\n",
       "Date   1998-06-27 00:00:00  1998-06-28 00:00:00  1998-06-29 00:00:00  \\\n",
       "id                                                                     \n",
       "11259                  0.0                  0.0                  0.0   \n",
       "7225                   0.0                  0.0                  0.0   \n",
       "16679                  0.0                  0.0                  0.0   \n",
       "23454                  0.0                  0.0                  0.0   \n",
       "18204                  0.0                  0.0                  0.0   \n",
       "\n",
       "Date   1998-06-30 00:00:00  \n",
       "id                          \n",
       "11259                  0.0  \n",
       "7225                   0.0  \n",
       "16679                  0.0  \n",
       "23454                  0.0  \n",
       "18204                  0.0  \n",
       "\n",
       "[5 rows x 549 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Prediction with predicted clusters on full data (2600 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predicted_labels = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets, test_targets= train_test_split(df_with_predicted_labels.transpose(), test_size=.127, shuffle=False)\n",
    "train_targets = train_targets.transpose()\n",
    "test_targets = test_targets.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 TRMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 7min 5s, total: 8min 13s\n",
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create TRMF Models\n",
    "for cluster_number in df_with_predicted_labels.clusters_r.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_r == cluster_number].drop(columns = ['clusters_m','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    \n",
    "    \n",
    "    # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = X.transpose()\n",
    "    X = transformer.fit_transform(X)\n",
    "    #GridSearchCV for TRMF\n",
    "    best = gridsearch_trmf(X,true_values)\n",
    "    #X = X.transpose()\n",
    "    \n",
    "    cluster = TRMFRegressor(**best, eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                     fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                     z_step_kind=\"tron\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster.fit(X)\n",
    "    #predicted = cluster.predict(X=X, n_ahead=n_ahead)\n",
    "    predicted = transformer.inverse_transform( cluster.predict(X=X, n_ahead=n_ahead))\n",
    "    X = X.transpose()\n",
    "    df_prediction = pd.DataFrame(predicted).transpose()\n",
    "    df_prediction.columns  = true_values.columns \n",
    "    df_prediction.index = true_values.index\n",
    "    \n",
    "    # Transform prediction for output\n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','trmf')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_R')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['id','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.4 s, sys: 3min 39s, total: 4min 16s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create TRMF Models\n",
    "for cluster_number in df_with_predicted_labels.clusters_m.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_m == cluster_number].drop(columns = ['clusters_r','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    \n",
    "    \n",
    "    # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = X.transpose()\n",
    "    X = transformer.fit_transform(X)\n",
    "    #GridSearchCV for TRMF\n",
    "    best = gridsearch_trmf(X,true_values)\n",
    "    #X = X.transpose()\n",
    "    \n",
    "    cluster = TRMFRegressor(**best, eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                     fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                     z_step_kind=\"tron\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster.fit(X)\n",
    "    #predicted = cluster.predict(X=X, n_ahead=n_ahead)\n",
    "    predicted = transformer.inverse_transform( cluster.predict(X=X, n_ahead=n_ahead))\n",
    "    X = X.transpose()\n",
    "    df_prediction = pd.DataFrame(predicted).transpose()\n",
    "    df_prediction.columns  = true_values.columns \n",
    "    df_prediction.index = true_values.index\n",
    "    \n",
    "    # Transform prediction for output\n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','trmf')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_M')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['id','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 7min 45s, total: 8min 59s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create TRMF Models\n",
    "for cluster_number in df_with_predicted_labels.clusters_f.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_f == cluster_number].drop(columns = ['clusters_r','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    \n",
    "    \n",
    "    # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = X.transpose()\n",
    "    X = transformer.fit_transform(X)\n",
    "    #GridSearchCV for TRMF\n",
    "    best = gridsearch_trmf(X,true_values)\n",
    "    #X = X.transpose()\n",
    "    \n",
    "    cluster = TRMFRegressor(**best, eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                     fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                     z_step_kind=\"tron\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    cluster.fit(X)\n",
    "    #predicted = cluster.predict(X=X, n_ahead=n_ahead)\n",
    "    predicted = transformer.inverse_transform( cluster.predict(X=X, n_ahead=n_ahead))\n",
    "    X = X.transpose()\n",
    "    df_prediction = pd.DataFrame(predicted).transpose()\n",
    "    df_prediction.columns  = true_values.columns \n",
    "    df_prediction.index = true_values.index\n",
    "    \n",
    "    # Transform prediction for output\n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','trmf')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','before_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_F')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['id','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.3 s, sys: 1.69 s, total: 53.9 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base = importr('forecast')\n",
    "# Create Theta Models\n",
    "for cluster_number in df_with_predicted_labels.clusters_r.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_r == cluster_number].drop(columns = ['clusters_m','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    # Scaling\n",
    "    scl =MinMaxScaler() #StandardScaler(with_mean=True, with_std=True)\n",
    "    X = scl.fit_transform(X)\n",
    "    #THETA\n",
    "    df_prediction = pd.DataFrame(columns = test_targets.columns)\n",
    "\n",
    "    train_targets_scaled = pd.DataFrame(X)\n",
    "    train_targets_scaled.index = true_values.index\n",
    "\n",
    "\n",
    "    for index, row in train_targets_scaled.iterrows():\n",
    "        res = robjects.FloatVector(row.drop(columns = ['uid']))\n",
    "        r_prediction = base.thetaf(res,70)[1]\n",
    "        df_prediction.loc[index] = r_prediction\n",
    "    #Transform prediction for output \n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','theta')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','after_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_R')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['index','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value','index':'id'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Monetary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.1 s, sys: 228 ms, total: 50.3 s\n",
      "Wall time: 50.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Theta Models\n",
    "for cluster_number in df_with_predicted_labels.clusters_m.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_m == cluster_number].drop(columns = ['clusters_r','clusters_f'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    # Scaling\n",
    "    scl =MinMaxScaler() #StandardScaler(with_mean=True, with_std=True)\n",
    "    X = scl.fit_transform(X)\n",
    "    #THETA\n",
    "    df_prediction = pd.DataFrame(columns = test_targets.columns)\n",
    "\n",
    "    train_targets_scaled = pd.DataFrame(X)\n",
    "    train_targets_scaled.index = true_values.index\n",
    "\n",
    "\n",
    "    for index, row in train_targets_scaled.iterrows():\n",
    "        res = robjects.FloatVector(row.drop(columns = ['uid']))\n",
    "        r_prediction = base.thetaf(res,70)[1]\n",
    "        df_prediction.loc[index] = r_prediction\n",
    "    #Transform prediction for output \n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','theta')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','after_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_M')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['index','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value','index':'id'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    \n",
    "    #break\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.6 s, sys: 1.66 s, total: 53.3 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create Theta Models\n",
    "for cluster_number in df_with_predicted_labels.clusters_f.unique():\n",
    "    X = train_targets.loc[train_targets.clusters_f == cluster_number].drop(columns = ['clusters_r','clusters_m'])\n",
    "    true_values = test_targets.loc[ X.index]\n",
    "    # Scaling\n",
    "    scl =MinMaxScaler() #StandardScaler(with_mean=True, with_std=True)\n",
    "    X = scl.fit_transform(X)\n",
    "    #THETA\n",
    "    df_prediction = pd.DataFrame(columns = test_targets.columns)\n",
    "\n",
    "    train_targets_scaled = pd.DataFrame(X)\n",
    "    train_targets_scaled.index = true_values.index\n",
    "\n",
    "\n",
    "    for index, row in train_targets_scaled.iterrows():\n",
    "        res = robjects.FloatVector(row.drop(columns = ['uid']))\n",
    "        r_prediction = base.thetaf(res,70)[1]\n",
    "        df_prediction.loc[index] = r_prediction\n",
    "    #Transform prediction for output \n",
    "    #MODEL\n",
    "    df_prediction.insert(0,'RMSE',math.sqrt(mean_squared_error(true_values,df_prediction)))\n",
    "    \n",
    "    df_prediction.insert(0,'model','theta')\n",
    "    #METHOD\n",
    "    df_prediction.insert(0,'method','clustered')\n",
    "    #STEP\n",
    "    df_prediction.insert(0,'step','after_classifier')\n",
    "    # CLUSTER\n",
    "    df_prediction.insert(0,'cluster_id','cluster_'+str(cluster_number)+'_F')\n",
    "    #Wide to logn\n",
    "    df_prediction = df_prediction.reset_index().melt(id_vars =['index','cluster_id','step','method','model','RMSE']).rename(index = str, columns={'variable':'date','value':'predicted_value','index':'id'})\n",
    "    true_values=true_values.reset_index().melt(id_vars=['id']).rename(index = str, columns={'variable':'date','value':'true_value'})\n",
    "    result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2940000, 9)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>step</th>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_value</th>\n",
       "      <th>true_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257325</th>\n",
       "      <td>15571</td>\n",
       "      <td>cluster_0_F</td>\n",
       "      <td>after_classifier</td>\n",
       "      <td>clustered</td>\n",
       "      <td>theta</td>\n",
       "      <td>2.091287</td>\n",
       "      <td>1998-06-22</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122073</th>\n",
       "      <td>9238</td>\n",
       "      <td>cluster_0_M</td>\n",
       "      <td>before_classifier</td>\n",
       "      <td>clustered</td>\n",
       "      <td>trmf</td>\n",
       "      <td>2.907609</td>\n",
       "      <td>1998-05-16</td>\n",
       "      <td>0.296918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25946</th>\n",
       "      <td>6943</td>\n",
       "      <td>cluster_0_M</td>\n",
       "      <td>before_classifier</td>\n",
       "      <td>clustered</td>\n",
       "      <td>trmf</td>\n",
       "      <td>2.907609</td>\n",
       "      <td>1998-04-27</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16414</th>\n",
       "      <td>17050</td>\n",
       "      <td>cluster_0_F</td>\n",
       "      <td>before_classifier</td>\n",
       "      <td>clustered</td>\n",
       "      <td>trmf</td>\n",
       "      <td>2.089774</td>\n",
       "      <td>1998-04-25</td>\n",
       "      <td>0.053962</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260061</th>\n",
       "      <td>18647</td>\n",
       "      <td>cluster_0_F</td>\n",
       "      <td>after_classifier</td>\n",
       "      <td>clustered</td>\n",
       "      <td>theta</td>\n",
       "      <td>2.091287</td>\n",
       "      <td>1998-06-23</td>\n",
       "      <td>-0.000512</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   cluster_id               step     method  model      RMSE  \\\n",
       "257325  15571  cluster_0_F   after_classifier  clustered  theta  2.091287   \n",
       "122073   9238  cluster_0_M  before_classifier  clustered   trmf  2.907609   \n",
       "25946    6943  cluster_0_M  before_classifier  clustered   trmf  2.907609   \n",
       "16414   17050  cluster_0_F  before_classifier  clustered   trmf  2.089774   \n",
       "260061  18647  cluster_0_F   after_classifier  clustered  theta  2.091287   \n",
       "\n",
       "             date  predicted_value  true_value  \n",
       "257325 1998-06-22        -0.000110         0.0  \n",
       "122073 1998-05-16         0.296918         0.0  \n",
       "25946  1998-04-27         0.030126         0.0  \n",
       "16414  1998-04-25         0.053962         0.0  \n",
       "260061 1998-06-23        -0.000512         0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(prediction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get full analysis, please run \"combine_predictions.ipynb\" file, which combines the prediction files of all approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
