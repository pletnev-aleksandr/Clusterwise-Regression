{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy.random import randint\n",
    "#!pip install git+git://github.com/ivannz/trmf\n",
    "from functools import reduce\n",
    "import math\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from trmf import TRMFRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error as mse_score\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.base import clone\n",
    "import scipy.sparse as ssp\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "#simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Use this file to feed into the clusterwise regression experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = 'Export/ICDM19/'\n",
    "#Train\n",
    "import_train_daily = 'import/M4/Daily-train.csv'\n",
    "import_train_weekly = 'import/M4/Weekly-train.csv'\n",
    "import_train_hourly = 'import/M4/Hourly-train.csv'\n",
    "import_train_monthly= 'import/M4/Monthly-train.csv'\n",
    "import_train_quartely = 'import/M4/Quarterly-train.csv'\n",
    "import_train_yearly = 'import/M4/Yearly-train.csv'\n",
    "#Test\n",
    "import_test_daily = 'import/M4/Daily-test.csv'\n",
    "import_test_weekly = 'import/M4/Weekly-test.csv'\n",
    "import_test_hourly = 'import/M4/Hourly-test.csv'\n",
    "import_test_monthly= 'import/M4/Monthly-test.csv'\n",
    "import_test_quartely = 'import/M4/Quarterly-test.csv'\n",
    "import_test_yearly = 'import/M4/Yearly-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters \n",
    "#import_clusters_daily = 'import/M4/Daily-test.csv'\n",
    "import_clusters_weekly = pd.read_csv('import/M4/Clusters/ensemble_clusterings_m4_weekly.csv')\n",
    "import_clusters_hourly = pd.read_csv('import/M4/Clusters/ensemble_clusterings_m4_hourly.csv')\n",
    "import_clusters_monthly= pd.read_csv('import/M4/Clusters/ensemble_clusterings_m4_monthly.csv')\n",
    "import_clusters_quartely = pd.read_csv('import/M4/Clusters/ensemble_clusterings_m4_quarterly.csv')\n",
    "import_clusters_yearly = pd.read_csv('import/M4/Clusters/ensemble_clusterings_m4_yearly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "#train_daily = pd.read_csv(import_train_daily)\n",
    "train_weekly = pd.read_csv(import_train_weekly)\n",
    "#train_hourly = pd.read_csv(import_train_hourly)\n",
    "#train_monthly= pd.read_csv(import_train_monthly)\n",
    "#train_quartely = pd.read_csv(import_train_quartely)\n",
    "#train_yearly = pd.read_csv(import_train_yearly)\n",
    "# Test\n",
    "#test_daily = pd.read_csv(import_test_daily)\n",
    "test_weekly = pd.read_csv(import_test_weekly)\n",
    "#test_hourly = pd.read_csv(import_test_hourly)\n",
    "#test_monthly= pd.read_csv(import_test_monthly)\n",
    "#test_quartely = pd.read_csv(import_test_quartely)   \n",
    "#test_yearly = pd.read_csv(import_test_yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut!\n",
    "train_weekly = train_weekly[:import_clusters_weekly.shape[0]]\n",
    "test_weekly = test_weekly[:import_clusters_weekly.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change TS so that the all end at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_move_nans(df):\n",
    "    \"\"\"\n",
    "    Moves nans in dataframe to the beginning\n",
    "    Ex:\n",
    "    1 2 3 nan\n",
    "    1 2 nan nan\n",
    "    Result:\n",
    "    nan   1 2 3\n",
    "    nan nan 1 2\n",
    "    \"\"\"\n",
    "    df_user = df.V1\n",
    "    # Temporarily discard column\n",
    "    df = df.drop(columns = ['V1'])\n",
    "    # Move nans to the begininng\n",
    "    df = df.transpose().apply(\n",
    "        (lambda row:  np.concatenate(\n",
    "            [np.full([  df.shape[1] - row.dropna().shape[0]], np.nan),\n",
    "                        row.dropna().values])) , axis = 0).transpose()\n",
    "    df.insert(0,'V1',df_user)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 12 ms, total: 532 ms\n",
      "Wall time: 534 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train_daily = dataframe_move_nans(train_daily).fillna(method ='backfill',axis = 1)\n",
    "train_weekly = dataframe_move_nans(train_weekly).fillna(method ='backfill',axis = 1)\n",
    "#train_hourly = dataframe_move_nans(train_hourly).fillna(method ='backfill',axis = 1)\n",
    "#train_monthly = dataframe_move_nans(train_monthly).fillna(method ='backfill',axis = 1)\n",
    "#train_quartely = dataframe_move_nans(train_quartely).fillna(method ='backfill',axis = 1)\n",
    "#train_yearly = dataframe_move_nans(train_yearly).fillna(method ='backfill',axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add clusters:\n",
    "##train_hourly.insert(1,'GMM_Pair',import_clusters_hourly.GMM_Pair_ff0)\n",
    "#train_hourly.insert(1,'GMM_Voting', import_clusters_hourly.GMM_Voting_ff0)\n",
    "\n",
    "train_weekly.insert(1,'GMM_Pair', import_clusters_weekly.GMM_Pair_ff0)\n",
    "train_weekly.insert(1,'GMM_Voting', import_clusters_weekly.GMM_Voting_ff0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster assignment (currently random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weekly.insert(1,'random initialization2',randint(0,max(import_clusters_yearly.GMM_Pair_ff0)-1,\n",
    "                                                       train_weekly.shape[0]))\n",
    "train_weekly.insert(1,'random initialization1',randint(0,max(import_clusters_yearly.GMM_Voting_ff0)-4,\n",
    "                                                       train_weekly.shape[0]))\n",
    "\n",
    "# COLUMN SHOULD BE IN THE BEGINNNG!\n",
    "#train_daily.insert(1,'random initialization1',randint(0,5,train_daily.shape[0]))\n",
    "#train_weekly.insert(1,'random initialization1',randint(0,5,train_weekly.shape[0]))\n",
    "\n",
    "#train_monthly.insert(1,'random initialization1',randint(0,5,train_monthly.shape[0]))\n",
    "#train_quartely.insert(1,'random initialization1',randint(0,5,train_quartely.shape[0]))\n",
    "#train_yearly.insert(1,'random initialization1',randint(0,5,train_yearly.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>random initialization1</th>\n",
       "      <th>random initialization2</th>\n",
       "      <th>GMM_Voting</th>\n",
       "      <th>GMM_Pair</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>...</th>\n",
       "      <th>V2589</th>\n",
       "      <th>V2590</th>\n",
       "      <th>V2591</th>\n",
       "      <th>V2592</th>\n",
       "      <th>V2593</th>\n",
       "      <th>V2594</th>\n",
       "      <th>V2595</th>\n",
       "      <th>V2596</th>\n",
       "      <th>V2597</th>\n",
       "      <th>V2598</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>W328</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3828</td>\n",
       "      <td>3828</td>\n",
       "      <td>3828</td>\n",
       "      <td>3828</td>\n",
       "      <td>3828</td>\n",
       "      <td>...</td>\n",
       "      <td>2549</td>\n",
       "      <td>2207</td>\n",
       "      <td>2060</td>\n",
       "      <td>1885</td>\n",
       "      <td>2039</td>\n",
       "      <td>2022</td>\n",
       "      <td>2332</td>\n",
       "      <td>2013</td>\n",
       "      <td>2382</td>\n",
       "      <td>2735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 2602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       V1  random initialization1  random initialization2  GMM_Voting  \\\n",
       "327  W328                       2                       3           6   \n",
       "\n",
       "     GMM_Pair    V2    V3    V4    V5    V6  ... V2589 V2590 V2591 V2592  \\\n",
       "327         6  3828  3828  3828  3828  3828  ...  2549  2207  2060  1885   \n",
       "\n",
       "    V2593 V2594 V2595 V2596 V2597 V2598  \n",
       "327  2039  2022  2332  2013  2382  2735  \n",
       "\n",
       "[1 rows x 2602 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weekly.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 2602)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weekly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe is ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Do hard clusterwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = TRMFRegressor(n_components=2,\n",
    "                 n_order=2,\n",
    "                 fit_regression=False,\n",
    "                 fit_intercept=True,\n",
    "                 nonnegative_factors=True,\n",
    "                 n_max_mf_iter=5)\n",
    "\n",
    "\n",
    "def gridsearch_trmf(X,test_targets):\n",
    "    grid = ParameterGrid(dict(\n",
    "    n_components=np.r_[1:2],#17],\n",
    "    n_order=np.r_[1:2],#17],\n",
    "    C_Z=np.logspace(-2, 1,num=1),#,num=4),\n",
    "    C_F=np.logspace(-2, 1,num=1),#,num=4),\n",
    "    C_phi=np.logspace(-2, 1,num=1),#,num=4),\n",
    "    eta_Z=np.linspace(0.05, 0.95,num=1)#,num=10),\n",
    "    ))\n",
    "    \n",
    "    transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "    X = transformer.fit_transform(X)\n",
    "\n",
    "    \n",
    "    # run the experiment in parallel\n",
    "    par_ = Parallel(n_jobs=-1, verbose=0)\n",
    "    results = par_(delayed(helper)(par, X, test_targets.transpose(),transformer) for par in grid)\n",
    "    \n",
    "    keys = ['n_order', 'n_components', 'eta_Z', 'C_phi', 'C_Z', 'C_F']\n",
    "    data = dict((tuple(par[k] for k in keys), rmse,) for par, rmse in results)\n",
    "    sr = pd.Series(data, name=\"rmse\").sort_index().rename_axis(keys)\n",
    "    cube = sr.values.reshape(*[len(grid.param_grid[0][k]) for k in keys])\n",
    "    stepping = [grid.param_grid[0][k] for k in keys]\n",
    "    # find the flat index of the smallest value\n",
    "    flat_index = np.argmin(cube)\n",
    "    # ... and unravel into into a multidimensional index\n",
    "    index = np.unravel_index(flat_index, cube.shape)\n",
    "    # collect the best paramaters from the grid\n",
    "    best_ = {k: grid.param_grid[0][k][i] for k, i in zip(keys, index)}\n",
    "    return best_\n",
    "\n",
    "transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "def helper(par, train, test,  transformer,base=base):\n",
    "    basee = TRMFRegressor(n_components=1,\n",
    "                     n_order=0,\n",
    "                     fit_regression=False,\n",
    "                     fit_intercept=True,\n",
    "                     nonnegative_factors=True,\n",
    "                     n_max_mf_iter=5)\n",
    "    \n",
    "    # clone, set parameters and fit\n",
    "    trmf = clone(basee).set_params(**par).fit(train)\n",
    "    \n",
    "    # predict and return\n",
    "    pred = transformer.inverse_transform(\n",
    "        trmf.predict(n_ahead=len(test)))\n",
    "\n",
    "    return par, mean_squared_error(test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true,y_pred,training_series= None): \n",
    "    weight = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    output = np.divide(np.abs(y_true - y_pred), weight,\n",
    "    where=weight > 0,\n",
    "    out=np.full_like(weight, np.nan))\n",
    "    return np.mean(np.nan_to_num(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_(y_true,y_pred,training_series= None): \n",
    "    weight = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    output = np.divide(np.abs(y_true - y_pred), weight,\n",
    "    where=weight > 0,\n",
    "    out=np.full_like(weight, np.nan))\n",
    "    return np.mean(np.nan_to_num(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinball_loss(A,F,training_series= None,tau = 0.49):\n",
    "    return np.mean(np.maximum(F - A, 0) * (1 - tau) +\n",
    "                 np.maximum(F - A, 0) * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred,training_series= None): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(testing_series, prediction_series,training_series):\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterwise_regression_sklearn(X,Y,regr,regr_gridsearch_params,\n",
    "                           cluster_column ='supercluster',\n",
    "                            horizon = True,\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           recalculate_assignment = 100,\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Runs clusterwise regression for sklearn functions\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    cluster_column : column with given clusters\n",
    "    horizon = :  True  - 1-step horizon (1 can be modified), False - n periods ahead\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    gridsearch_limit : # of times to run GridSeach for each cluster (>=1)\n",
    "    recalculate_assignment : # of assignments after which the clusters will be recalculated\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    clusters : Dictionary with trained models for each cluster\n",
    "    old_cluster_for_user : Dictionary with optimal cluster for each user (after moving)\n",
    "    cluster_parameters_for_gridsearch : Dictionary with optimal parameters for each cluster model\n",
    "    weights\n",
    "    \"\"\"   \n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()  \n",
    "    n_ahead = test_targets.shape[1]\n",
    "    clusters = {}\n",
    "    # save cluster info for clusterwise regression\n",
    "    cluster_error = {}\n",
    "    cluster_has_changed = {}\n",
    "    cluster_data_X = {}\n",
    "    counter_for_gridsearch = 0\n",
    "    old_clusters ={}\n",
    "    cluster_data_true_values = {}\n",
    "    cluster_predicted = {}\n",
    "    score_for_user = {}\n",
    "    cluster_parameters_for_gridsearch = {}\n",
    "    previous_total_error = math.inf\n",
    "    limit = 50\n",
    "    for i in list(range(0,limit+1)): # Some limit on iterations just in case\n",
    "        # Assign user to clusters\n",
    "        cluster_for_user = {}\n",
    "        cluster_for_user  = train_targets[cluster_column].to_dict()\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            # Save training data for this cluster\n",
    "            \n",
    "            cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "            cluster_data_true_values[cluster_number] = test_targets.loc[ cluster_data_X[cluster_number].index]\n",
    "            cluster_predicted[cluster_number] =pd.DataFrame()\n",
    "            if (cluster_number not in cluster_has_changed) or (cluster_has_changed[cluster_number]== True):\n",
    "                    clusters[cluster_number] = clone(regr)\n",
    "                    cluster_predicted[cluster_number] =pd.DataFrame()\n",
    "            for slide in list(range(0,test_targets.shape[1])):\n",
    "                #Train\n",
    "                transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "                #print(\"X_train = \",cluster_data_X[cluster_number].iloc[:,:-1].columns)\n",
    "                fitted_ = transformer.fit_transform(cluster_data_X[cluster_number].iloc[:,:-1]) # X - all col except last one\n",
    "                #print(\"y_train = \",cluster_data_X[cluster_number].iloc[:,-1:].columns)\n",
    "                #print(cluster_data_X[cluster_number].iloc[:,-1:])\n",
    "                clusters[cluster_number].fit(fitted_,cluster_data_X[cluster_number].iloc[:,-1:].values) # y - last column\n",
    "                #Test\n",
    "                #delete first col\n",
    "                cluster_data_X[cluster_number] = cluster_data_X[cluster_number].drop(cluster_data_X[cluster_number].columns[0], axis=1)  \n",
    "                #print(\"X_test = \",cluster_data_X[cluster_number].columns)\n",
    "                fitted_ = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "                cluster_predicted[cluster_number][slide] =clusters[cluster_number].predict(fitted_).ravel() \n",
    "                #add new one at the end (what we've just predicted)\n",
    "                if (horizon == False):\n",
    "                    cluster_data_X[cluster_number][slide] = cluster_predicted[cluster_number][slide].values \n",
    "                else:\n",
    "                    cluster_data_X[cluster_number][slide] =(cluster_data_true_values[cluster_number].iloc[:,slide])   # true values\n",
    "                #print(\"y_true = \",cluster_data_true_values[cluster_number].columns[slide])\n",
    "           # return cluster_predicted           \n",
    "            # Save prediction\n",
    "            cluster_predicted[cluster_number] = pd.DataFrame(cluster_predicted[cluster_number])\n",
    "\n",
    "            cluster_predicted[cluster_number].columns  = cluster_data_true_values[cluster_number].columns \n",
    "            cluster_predicted[cluster_number].index = cluster_data_true_values[cluster_number].index\n",
    "            #save cluster score\n",
    "            cluster_error[cluster_number] =  score_function(cluster_data_true_values[cluster_number].values.ravel(),cluster_predicted[cluster_number].values.ravel())\n",
    "        print(\"Error per cluster: \",cluster_error)\n",
    "        # flatten true values\n",
    "        all_true_values  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_data_true_values.values())  ]])   )\n",
    "        # flatten prediction \n",
    "        all_predictions  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_predicted.values())  ]])   )\n",
    "        total_error =  score_function(np.array(all_true_values),np.array(all_predictions))\n",
    "        print(\"New error: \",round(total_error,4))\n",
    "        # If new score is worse - load last cluster and run final prediction.\n",
    "        if (total_error > previous_total_error):\n",
    "            if counter_iterations ==3:\n",
    "               # train_targets[cluster_column] = list(old_cluster_for_user.values())\n",
    "                print('Error has increased. Backtrack to previous iteration')\n",
    "                return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i-counter_iterations\n",
    "            counter_iterations+=1\n",
    "        else:\n",
    "            counter_iterations = 0 \n",
    "            previous_total_error = total_error\n",
    "            old_clusters = clusters.copy()\n",
    "            old_cluster_for_user = cluster_for_user.copy()\n",
    "       \n",
    "        if (i>=limit):# return prev iteration\n",
    "            return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i\n",
    "        # Calculate score per user\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():\n",
    "                score_for_user[index] = score_function(row, cluster_predicted[cluster_number].loc[index,:])\n",
    "        # Assume that we haven't change any cluster\n",
    "        for x in cluster_has_changed:\n",
    "            cluster_has_changed[x]=False        \n",
    "        cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "       # old_clusters = {}\n",
    "       # old_cluster_for_user = {}\n",
    "        ii =0\n",
    "        reassignment_count = 0 \n",
    "        if (i==0):\n",
    "            old_clusters = clusters.copy()\n",
    "        #Take observation x from cluster a, where error_x > error_a\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            # All ids in this cluster are already in cluster with lowest possible mae\n",
    "            if (cluster_number == cluster_lowest_mae):\n",
    "                continue\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():    \n",
    "            # User is in current cluster -> we can reassign him\n",
    "                if (cluster_number  == cluster_for_user[index]):\n",
    "                    if (score_for_user[index] > cluster_error[cluster_number]):\n",
    "                        ii+=1#counter\n",
    "                        # Have to refit changed clusters             \n",
    "                        cluster_has_changed[cluster_number] = True\n",
    "                        cluster_has_changed[cluster_lowest_mae] = True\n",
    "                        # Move observation x to the cluster with the lowest training erro\n",
    "                        train_targets.loc[index,cluster_column] = cluster_lowest_mae\n",
    "                        reassignment_count+=1\n",
    "                        if reassignment_count==recalculate_assignment:\n",
    "                            reassignment_count=0\n",
    "                            \n",
    "                            # RECALCULATION - only best performing cluster (because)- TODO:save snapshot of prev cluster for backtracking\n",
    "                            cluster_data_X_reassigned =  train_targets.loc[train_targets[cluster_column] == cluster_lowest_mae].drop(columns =columns_wo_TS)\n",
    "                            cluster_data_true_values_reassigned= test_targets.loc[ cluster_data_X_reassigned.index]\n",
    "                           \n",
    "                            clusters_reassignemnt = clone(clusters[cluster_lowest_mae])\n",
    "                            cluster_predicted_reassigned = pd.DataFrame()\n",
    "                            for slide in list(range(0,test_targets.shape[1])):\n",
    "                                #Train\n",
    "                                transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "       \n",
    "                                fitted_ = transformer.fit_transform(cluster_data_X_reassigned.iloc[:,:-1]) # X - all col except last one\n",
    "                                clusters_reassignemnt.fit(fitted_,cluster_data_X_reassigned.iloc[:,-1:].values) # y - last column\n",
    "                                #Test\n",
    "                                #delete first col\n",
    "                                cluster_data_X_reassigned = cluster_data_X_reassigned.drop(cluster_data_X_reassigned.columns[0], axis=1)  \n",
    "                                predd = transformer.fit_transform(cluster_data_X_reassigned)\n",
    "                                cluster_predicted_reassigned[slide] =clusters[cluster_number].predict(predd).ravel() \n",
    "                                #add new one at the end (what we've just predicted)\n",
    "                                if horizon == False:\n",
    "                                    cluster_data_X_reassigned[slide] = cluster_predicted_reassigned[slide].values\n",
    "                                else:\n",
    "                                    cluster_data_X_reassigned[slide] =(cluster_data_true_values_reassigned.iloc[:,slide]) \n",
    "                            # Save prediction\n",
    "                            cluster_predicted_reassigned = pd.DataFrame(cluster_predicted_reassigned)\n",
    "                            cluster_error[cluster_lowest_mae] = score_function(cluster_data_true_values_reassigned.values.ravel(),cluster_predicted_reassigned.values.ravel())\n",
    "                            cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "        if (i==0):\n",
    "            old_cluster_for_user = cluster_for_user.copy() # as well as save last assigned clusters                  \n",
    "        print('Moved ', ii, ' users to another cluster \\n')   \n",
    "   # return old_clusters,old_cluster_for_user,cluster_parameters_for_gridsearch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prediction_after_clusterwise_reg_sklearn(\n",
    "                           X,Y,regr,\n",
    "    dataset,\n",
    "    method,\n",
    "    frequency,\n",
    "    model,\n",
    "                            horizon= True,\n",
    "                           step = 'unstated',\n",
    "                           clusters = None,\n",
    "                           clusters_parameters = None,\n",
    "                           new_clusters_for_users=None,\n",
    "                           cluster_column ='supercluster',\n",
    "                           scores_per_cluster = True,\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                           gridsearch_use = False,):\n",
    "    \"\"\"\n",
    "    Performs final prediction and generates DataFrame for Alibaba Data\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    horizon : stated previously\n",
    "    step : 'before_classifier' or 'after_classifier', for report generation\n",
    "    clusters: dictionary with models for each cluster. If None, then refit\n",
    "    clusters_parameters: dictionary with parameters for each cluster model\n",
    "    new_clusters_for_users: dictionary with clusters for each user. \n",
    "    cluster_column : column with given clusters\n",
    "    \n",
    "    scores_per_cluster : if False the score will be given for the all dataset, not by cluster\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    gridsearch_use : Run gridSeach (True) or use clusters_parameters (False)?\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    result : DataFrame with columns\n",
    "    cluster, user id, true target, prediction, rmse, model, method (all data, clustered), step (all data, before classifier, after classifier)\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()\n",
    "    n_ahead = test_targets.shape[1]\n",
    "    if (new_clusters_for_users is not None):\n",
    "        # we assume that the order in DataFrame is the same as in clusterwise regression\n",
    "        train_targets[cluster_column]=list(cluster_per_user.values()) \n",
    "    cluster_data_X = {}\n",
    "    cluster_data_true_values = {}\n",
    "    if (clusters_parameters == None or gridsearch_use==True):\n",
    "        clusters_parameters = {}\n",
    "    if (clusters == None or clusters_parameters ==None):\n",
    "        clusters = {}\n",
    "    for cluster_number in train_targets[cluster_column].unique():\n",
    "        cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "        cluster_data_true_values[cluster_number] = test_targets.iloc[ cluster_data_X[cluster_number].index]\n",
    "        cluster_data_true_values[cluster_number].index =   test_targets.iloc[ cluster_data_X[cluster_number].index].index\n",
    "        predicted =pd.DataFrame()\n",
    "        \n",
    "        #if (cluster_number not in clusters or ( cluster_number not in  clusters_parameters) or gridsearch_use==True):\n",
    "        clusters[cluster_number] = clone(regr)\n",
    "            \n",
    "        for slide in list(range(0,test_targets.shape[1])):\n",
    "            #Train\n",
    "            transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "            #print(\"X_train = \",cluster_data_X[cluster_number].iloc[:,:-1].columns)\n",
    "            fitted_ = transformer.fit_transform(cluster_data_X[cluster_number].iloc[:,:-1]) # X - all col except last one\n",
    "\n",
    "            clusters[cluster_number].fit(fitted_,cluster_data_X[cluster_number].iloc[:,-1:].values) # y - last column\n",
    "            #Test\n",
    "            #delete first col\n",
    "            cluster_data_X[cluster_number] = cluster_data_X[cluster_number].drop(cluster_data_X[cluster_number].columns[0], axis=1)  \n",
    "            #print(\"X_test = \",cluster_data_X[cluster_number].columns)\n",
    "            fitted_ = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "            predicted[slide] =clusters[cluster_number].predict(fitted_).ravel() \n",
    "            #add new one at the end (what we've just predicted)\n",
    "            if (horizon == False):\n",
    "                cluster_data_X[cluster_number][slide] = predicted[slide].values \n",
    "            else:\n",
    "              #  print(cluster_data_true_values[cluster_number].iloc[:,slide])\n",
    "                cluster_data_X[cluster_number][slide] =(cluster_data_true_values[cluster_number].iloc[:,slide]).values   # true values\n",
    "        \n",
    "        \n",
    "        df_prediction = pd.DataFrame(predicted)\n",
    "        df_prediction.columns  = cluster_data_true_values[cluster_number].columns \n",
    "        df_prediction.index = cluster_data_true_values[cluster_number].index\n",
    "       # df_prediction.index =  train_targets.loc[train_targets[cluster_column] == cluster_number][\"Unnamed: 1\"]\n",
    "        # Transform prediction for output\n",
    "        #MODEL        \n",
    "        \n",
    "        df_prediction.insert(0,'Score',score_function(cluster_data_true_values[cluster_number].values.ravel(),df_prediction.values.ravel(),training_series=fitted_))\n",
    "        df_prediction.insert(0,'model',model)\n",
    "        #Dataset\n",
    "        df_prediction.insert(0,'dataset',dataset)\n",
    "        #METHOD\n",
    "        df_prediction.insert(0,'method',method)\n",
    "        #STEP\n",
    "        df_prediction.insert(0,'frequency',frequency)\n",
    "        if horizon == True:\n",
    "            df_prediction.insert(0,'horizon',str(1)+'_step')\n",
    "        else:\n",
    "            df_prediction.insert(0,'horizon','full')\n",
    "        # CLUSTER\n",
    "        df_prediction.insert(0,'cluster_id',str(cluster_column)+'_'+str(cluster_number))\n",
    "        #Wide to logn\n",
    "        #print(pd.DataFrame(df_prediction))\n",
    "        df_prediction = df_prediction.reset_index().melt(id_vars =['V1','cluster_id','model','horizon','dataset','method','frequency','Score']).rename(index = str, columns={'variable':'date','value':'predicted_value','V1':'id'})\n",
    "        true_values=cluster_data_true_values[cluster_number].reset_index().melt(id_vars=['V1']).rename(index = str, columns={'variable':'date','value':'true_value','V1':'id'})\n",
    "        result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))\n",
    "    if scores_per_cluster == False:\n",
    "           result.Score = score_function(result.true_value,result.predicted_value,\n",
    "                             training_series=train_targets.drop(columns =columns_wo_TS).values.ravel())\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterwise_regression(X,Y,\n",
    "                           cluster_column ='supercluster',\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           gridsearch_limit = 1,\n",
    "                           recalculate_assignment = 100,\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Runs clusterwise regression\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    cluster_column : column with given clusters\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    gridsearch_limit : # of times to run GridSeach for each cluster (>=1)\n",
    "    recalculate_assignment : # of assignments after which the clusters will be recalculated\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    clusters : Dictionary with trained models for each cluster\n",
    "    old_cluster_for_user : Dictionary with optimal cluster for each user (after moving)\n",
    "    cluster_parameters_for_gridsearch : Dictionary with optimal parameters for each cluster model\n",
    "    weights\n",
    "    \"\"\"\n",
    "        \n",
    "        \n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()  # test_Y \n",
    "    n_ahead = test_targets.shape[1]\n",
    "    clusters = {}\n",
    "    # save cluster info for clusterwise regression\n",
    "    cluster_error = {}\n",
    "    cluster_has_changed = {}\n",
    "    cluster_data_X = {}\n",
    "    counter_for_gridsearch = 0\n",
    "    old_clusters ={}\n",
    "    counter_iterations = 0 \n",
    "    cluster_data_true_values = {}\n",
    "    cluster_predicted = {}\n",
    "    score_for_user = {}\n",
    "    cluster_parameters_for_gridsearch = {}\n",
    "    previous_total_error = math.inf\n",
    "    limit = 50\n",
    "    for i in list(range(0,limit+1)): # Some limit on iterations just in case\n",
    "        # Assign user to clusters\n",
    "        cluster_for_user = {}\n",
    "        cluster_for_user  = train_targets[cluster_column].to_dict()\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            cluster_predicted[cluster_number] =pd.DataFrame()\n",
    "            # Save training data for this cluster\n",
    "            cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "            cluster_data_true_values[cluster_number] = test_targets.loc[ cluster_data_X[cluster_number].index]\n",
    "            # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "            transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "            cluster_data_X[cluster_number] = cluster_data_X[cluster_number].transpose()\n",
    "           # print(cluster_number)\n",
    "           # print(cluster_data_X[cluster_number])\n",
    "            cluster_data_X[cluster_number] = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "            #GridSearchCV for TRMF\n",
    "            # If initial initialization or cluster has changed\n",
    "            if (cluster_number not in cluster_has_changed) or (cluster_has_changed[cluster_number]== True):\n",
    "                if (i<gridsearch_limit):\n",
    "                    # add prev_cluster_paraeters\n",
    "                    cluster_parameters_for_gridsearch[cluster_number] = gridsearch_trmf(cluster_data_X[cluster_number],cluster_data_true_values[cluster_number])\n",
    "                clusters[cluster_number ]= TRMFRegressor(**cluster_parameters_for_gridsearch[cluster_number], eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                                 fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                                 z_step_kind=\"tron\")\n",
    "                clusters[cluster_number].fit(cluster_data_X[cluster_number])\n",
    "            fitted_ = np.dot(clusters[cluster_number].factors_,clusters[cluster_number].loadings_)+clusters[cluster_number].intercept_\n",
    "            cluster_predicted[cluster_number] = transformer.inverse_transform( clusters[cluster_number].predict(fitted_,n_ahead))\n",
    "            \n",
    "            # Save prediction\n",
    "            cluster_predicted[cluster_number] = pd.DataFrame(cluster_predicted[cluster_number]).transpose()\n",
    "            \n",
    "           # print(cluster_predicted[cluster_number].columns )\n",
    "           # print( cluster_data_true_values[cluster_number].columns)\n",
    "            cluster_predicted[cluster_number].columns  = cluster_data_true_values[cluster_number].columns \n",
    "            cluster_predicted[cluster_number].index = cluster_data_true_values[cluster_number].index\n",
    "            #save cluster score\n",
    "            cluster_error[cluster_number] =  score_function(cluster_data_true_values[cluster_number].values.ravel(),cluster_predicted[cluster_number].values.ravel())\n",
    "        print(\"Error per cluster: \",cluster_error)\n",
    "        # flatten true values\n",
    "        all_true_values  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_data_true_values.values())  ]])   )\n",
    "        # flatten prediction \n",
    "        all_predictions  = reduce(lambda x,y: list(x)+list(y), ([x.ravel() for x in [x.values for x in list(cluster_predicted.values())  ]])   )\n",
    "        total_error =  score_function(np.array(all_true_values),np.array(all_predictions))\n",
    "        print(\"New error: \",round(total_error,4))\n",
    "        # If new score is worse - load last cluster and run final prediction. \n",
    "        if (total_error > previous_total_error):\n",
    "            if counter_iterations ==3:\n",
    "               # train_targets[cluster_column] = list(old_cluster_for_user.values())\n",
    "                print('Error has increased. Backtrack to previous iteration')\n",
    "                return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i-counter_iterations\n",
    "            counter_iterations+=1\n",
    "        else:\n",
    "            counter_iterations = 0 \n",
    "            previous_total_error = total_error\n",
    "            old_clusters = clusters.copy()\n",
    "            old_cluster_for_user = cluster_for_user.copy()\n",
    "            # Save all stuff\n",
    "        if (i>=limit):# return prev iteration\n",
    "            return old_clusters.copy(),old_cluster_for_user,cluster_parameters_for_gridsearch,i\n",
    "        # Calculate score per user\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():\n",
    "                score_for_user[index] = score_function(row, cluster_predicted[cluster_number].loc[index,:])\n",
    "        # Assume that we haven't change any cluster\n",
    "        for x in cluster_has_changed:\n",
    "            cluster_has_changed[x]=False        \n",
    "        cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "       # old_clusters = {}\n",
    "        #old_cluster_for_user = {}\n",
    "        ii =0\n",
    "        reassignment_count = 0\n",
    "        if (i==0):\n",
    "            old_clusters = clusters.copy()\n",
    "        #Take observation x from cluster a, where error_x > error_a\n",
    "        for cluster_number in train_targets[cluster_column].unique():\n",
    "            # All ids in this cluster are already in cluster with lowest possible mae\n",
    "            if (cluster_number == cluster_lowest_mae):\n",
    "                continue\n",
    "            for index, row in cluster_data_true_values[cluster_number].iterrows():    \n",
    "            # User is in current cluster -> we can reassign him\n",
    "                if (cluster_number  == cluster_for_user[index]):\n",
    "                    if (score_for_user[index] > cluster_error[cluster_number]):\n",
    "                        ii+=1#counter\n",
    "                        # Have to refit changed clusters             \n",
    "                        cluster_has_changed[cluster_number] = True\n",
    "                        cluster_has_changed[cluster_lowest_mae] = True\n",
    "                        # Move observation x to the cluster with the lowest training erro\n",
    "                        train_targets.loc[index,cluster_column] = cluster_lowest_mae\n",
    "                        reassignment_count+=1\n",
    "                        if reassignment_count==recalculate_assignment:\n",
    "                            reassignment_count=0\n",
    "                            # RECALCULATION - only best performing cluster (because)- TODO:save snapshot of prev cluster for backtracking\n",
    "                            cluster_data_X_reassigned =  train_targets.loc[train_targets[cluster_column] == cluster_lowest_mae].drop(columns =columns_wo_TS)\n",
    "                            \n",
    "                            # print(  train_targets.loc[train_targets.cluster==2].drop(columns = ['cluster','V1']).shape)\n",
    "                            cluster_data_true_values_reassigned= test_targets.loc[ cluster_data_X_reassigned.index]\n",
    "                            cluster_data_X_reassigned =  transformer.fit_transform(cluster_data_X_reassigned.transpose())\n",
    "                            clusters_reassignemnt = clone(clusters[cluster_lowest_mae])\n",
    "                            clusters_reassignemnt.fit(cluster_data_X_reassigned)\n",
    "                            predd =  transformer.inverse_transform(clusters_reassignemnt.predict(cluster_data_X_reassigned ,n_ahead=n_ahead))\n",
    "                            cluster_predicted_reassigned = predd\n",
    "                            # Save prediction\n",
    "                            cluster_predicted_reassigned = pd.DataFrame(cluster_predicted_reassigned).transpose()\n",
    "                            cluster_error[cluster_lowest_mae] = score_function(cluster_data_true_values_reassigned.values.ravel(),cluster_predicted_reassigned.values.ravel())\n",
    "                            cluster_lowest_mae = min(cluster_error, key=cluster_error.get)\n",
    "        if (i==0):\n",
    "            old_cluster_for_user = cluster_for_user.copy() # as well as save last assigned clusters                  \n",
    "        print('Moved ', ii, ' users to another cluster \\n')   \n",
    "   # return old_clusters,old_cluster_for_user,cluster_parameters_for_gridsearch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prediction_after_clusterwise_reg(\n",
    "    X,Y,\n",
    "        dataset,\n",
    "    method,\n",
    "    frequency,\n",
    "    model,           \n",
    "                           step = 'unstated',\n",
    "                           clusters = None,\n",
    "                           clusters_parameters = None,\n",
    "                           new_clusters_for_users=None,\n",
    "                           cluster_column ='supercluster',\n",
    "                           scores_per_cluster = True,\n",
    "                           columns_wo_TS = ['clusters_r','clusters_m','clusters_f','supercluster'],\n",
    "                           score_function = (lambda x,y: math.sqrt(mean_squared_error(x,y))),\n",
    "                           gridsearch_use = False):\n",
    "    \"\"\"\n",
    "    Performs final prediction and generates DataFrame for Alibaba Data\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame with train data (rows: id; columns: clusters + TS)\n",
    "    Y : DataFrame with true_values (rows: id; columns: clusters + TS) (to compare predictions)\n",
    "    step : 'before_classifier' or 'after_classifier', for report generation\n",
    "    clusters: dictionary with models for each cluster. If None, then refit\n",
    "    clusters_parameters: dictionary with parameters for each cluster model\n",
    "    new_clusters_for_users: dictionary with clusters for each user. \n",
    "    cluster_column : column with given clusters\n",
    "    \n",
    "    scores_per_cluster : if False the score will be given for the all dataset, not by cluster\n",
    "    columns_wo_TS  : columns in DataFrames not regarded as TimeSeries\n",
    "    score_function : smape/ pinball_loss/ mape/(lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "    gridsearch_use : Run gridSeach (True) or use clusters_parameters (False)?\n",
    "    regr : if not tmrf, then use sklearn api\n",
    "    regr_gridsearch_params : if not tmrf, then use sklearn api for gridsearchCV\n",
    "    ---------\n",
    "    Return :\n",
    "    result : DataFrame with columns\n",
    "    cluster, user id, true target, prediction, rmse, model, method (all data, clustered), step (all data, before classifier, after classifier)\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    train_targets = X.copy()\n",
    "    test_targets = Y.copy()\n",
    "    n_ahead = test_targets.shape[1]\n",
    "    if (new_clusters_for_users is not None):\n",
    "        # we assume that the order in DataFrame is the same as in clusterwise regression\n",
    "        train_targets[cluster_column]=list(cluster_per_user.values()) \n",
    "    cluster_data_X = {}\n",
    "    cluster_data_true_values = {}\n",
    "    if (clusters_parameters == None or gridsearch_use==True):\n",
    "        clusters_parameters = {}\n",
    "    if (clusters == None or clusters_parameters ==None):\n",
    "        clusters = {}\n",
    "    for cluster_number in train_targets[cluster_column].unique():\n",
    "        cluster_data_X[cluster_number] = train_targets.loc[train_targets[cluster_column] == cluster_number].drop(columns =columns_wo_TS)\n",
    "        cluster_data_true_values[cluster_number] = test_targets.iloc[ cluster_data_X[cluster_number].index]\n",
    "        cluster_data_true_values[cluster_number].index =   test_targets.iloc[ cluster_data_X[cluster_number].index].index\n",
    "        # Scaling (Not minmax because of it wasn't used in trmf github)\n",
    "        transformer = StandardScaler(with_mean=True, with_std=True)\n",
    "        \n",
    "        cluster_data_X[cluster_number] = cluster_data_X[cluster_number].transpose()\n",
    "        cluster_data_X[cluster_number] = transformer.fit_transform(cluster_data_X[cluster_number])\n",
    "        if (cluster_number not in clusters_parameters or gridsearch_use==True):\n",
    "            clusters_parameters[cluster_number] = gridsearch_trmf(cluster_data_X[cluster_number],cluster_data_true_values[cluster_number])\n",
    "                    \n",
    "        if (cluster_number not in clusters or ( cluster_number not in  clusters_parameters) or gridsearch_use==True):\n",
    "            clusters[cluster_number ]= TRMFRegressor(**clusters_parameters[cluster_number], eta_F=0., adj=None, C_B=0., fit_regression=False,\n",
    "                             fit_intercept=True, nonnegative_factors=True, n_max_mf_iter=5,\n",
    "                             z_step_kind=\"tron\")\n",
    "                # If you feed clusters from clusterwise, you don't need to refit them\n",
    "            clusters[cluster_number ].fit(cluster_data_X[cluster_number])\n",
    "        fitted_ = np.dot(clusters[cluster_number].factors_,clusters[cluster_number].loadings_)+clusters[cluster_number].intercept_\n",
    "        predicted = transformer.inverse_transform( clusters[cluster_number].predict(fitted_,n_ahead))\n",
    "\n",
    "            \n",
    "        df_prediction = pd.DataFrame(predicted).transpose()\n",
    "        df_prediction.columns  = cluster_data_true_values[cluster_number].columns \n",
    "        df_prediction.index = cluster_data_true_values[cluster_number].index # here we need to set old cluster\n",
    "    \n",
    "        # Transform prediction for output\n",
    "        #MODEL\n",
    "        df_prediction.insert(0,'Score',score_function(cluster_data_true_values[cluster_number].values.ravel(),df_prediction.values.ravel(),training_series=fitted_))\n",
    "        df_prediction.insert(0,'model',model)\n",
    "        #Dataset\n",
    "        df_prediction.insert(0,'dataset',dataset)\n",
    "        #METHOD\n",
    "        df_prediction.insert(0,'method',method)\n",
    "        #STEP\n",
    "        df_prediction.insert(0,'frequency',frequency)\n",
    "        df_prediction.insert(0,'horizon','full')\n",
    "        df_prediction.insert(0,'cluster_id',str(cluster_column)+'_'+str(cluster_number))\n",
    "        #Wide to logn\n",
    "        df_prediction = df_prediction.reset_index().melt(id_vars =['V1','cluster_id','model','horizon','dataset','method','frequency','Score']).rename(index = str, columns={'variable':'date','value':'predicted_value','V1':'id'})\n",
    "        true_values=cluster_data_true_values[cluster_number].reset_index().melt(id_vars=['V1']).rename(index = str, columns={'variable':'date','value':'true_value','V1':'id'})\n",
    "        result = result.append(df_prediction.merge(true_values,left_on = ['id','date'],right_on=['id','date']))\n",
    "    if scores_per_cluster == False:\n",
    "           result.Score = score_function(result.true_value,result.predicted_value,\n",
    "                             training_series=train_targets.drop(columns =columns_wo_TS).values.ravel())\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if it can work with Gridsearch CV(It should but, you can just pass None)\n",
    "class RegressorEnsemble(BaseEstimator):\n",
    "    def __init__(self, rgr, n_estimators=10):\n",
    "        self.rgr = rgr\n",
    "        self.n_estimators = n_estimators\n",
    "        self.rgrs = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.rgrs.append(clone(self.rgr))\n",
    "\n",
    "    def fit(self, X, y,\n",
    "              seed=None,):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        #sd = np.random()\n",
    "        #print(sd)\n",
    "        for i in range(self.n_estimators):\n",
    "            self.rgrs[i].fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        ans = self.rgrs[0].predict(X)\n",
    "        for i in range(1,self.n_estimators):\n",
    "            ans += self.rgrs[i].predict(X)\n",
    "        return ans / len(self.rgrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,max_depth=3,\n",
    "                                                          random_state=0))\n",
    "regr_multirf = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',\n",
    "                                                     n_estimators=50)) #tree_method='gpu_hist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_yearly.shape  - okay shape (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 2602)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weekly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 14)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weekly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_column_limit = 2575 # 1 - without deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error per cluster:  {3: 90.70531176539522, 4: 127.23504885304376, 5: 145.65682978846036, 0: 182.86138600421285, 6: 297.09073385761883, 1: 173.55534017533316, 2: 167.51755607731442}\n",
      "New error:  165.6743\n",
      "Moved  81  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 162.80927445479358, 4: 102.8289667293589, 5: 203.43471020070496, 0: 205.20499727347018, 6: 122.46548880951458, 1: 216.72305726528822, 2: 142.0200746665872}\n",
      "New error:  166.1038\n",
      "Moved  51  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 87.26797709885535, 4: 116.73736163980654, 5: 170.23448256152068, 0: 227.10013562741818, 6: 185.27036710220727, 1: 257.4230385547614, 2: 216.80056837078155}\n",
      "New error:  177.5924\n",
      "Moved  35  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 82.94585730564334, 4: 112.28123278260452, 5: 118.4969365645506, 0: 116.94306274790475, 6: 192.97470146037313, 1: 274.9622920226908, 2: 239.25294496252613}\n",
      "New error:  166.505\n",
      "Moved  56  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 131.8197714576499, 4: 134.96095923738184, 5: 122.90775782522931, 0: 120.81768543871051, 6: 122.77020852046273, 1: 203.31444801051686, 2: 264.8117180447207}\n",
      "New error:  161.3831\n",
      "Moved  69  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 82.49722952521576, 4: 268.0195875341705, 5: 142.53933309622232, 0: 116.43180734623861, 6: 145.0608980014348, 1: 278.096547074851, 2: 288.24899830901506}\n",
      "New error:  195.2381\n",
      "Moved  54  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 117.40784269524902, 4: 109.16586595922328, 5: 98.27722201652483, 0: 162.831666196584, 6: 262.3789782447311, 1: 379.084382620614, 2: 74.5326395780165}\n",
      "New error:  175.4806\n",
      "Moved  85  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 146.62280041857193, 4: 88.08350503083116, 5: 194.97339920259333, 0: 142.4807724464405, 6: 253.2735697177985, 1: 238.521995520077, 2: 162.62664232796013}\n",
      "New error:  165.0598\n",
      "Moved  33  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 162.14467463838696, 4: 106.60862627307566, 5: 233.47764428831348, 0: 159.58527766179967, 6: 72.35675587382119, 1: 313.77515889668666, 2: 213.35736034829313}\n",
      "New error:  180.867\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(42003, 14)\n",
      "(46670, 14)\n",
      "Error per cluster:  {2: 205.30770031613898, 0: 265.37987498421967, 1: 289.93208764368757, 3: 85.34078349732128}\n",
      "New error:  212.2441\n",
      "Moved  88  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 166.67480729182117, 0: 138.55068513215838, 1: 248.89578217816188, 3: 123.04790950754109}\n",
      "New error:  169.5194\n",
      "Moved  24  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 188.72546098750502, 0: 151.88246861060267, 1: 224.54963746812484, 3: 153.101064305702}\n",
      "New error:  180.5977\n",
      "Moved  36  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 109.47454476567209, 0: 213.92592677468298, 1: 230.63296765831353, 3: 195.77658767633386}\n",
      "New error:  185.9284\n",
      "Moved  42  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 160.2188281164823, 0: 161.5664235475535, 1: 253.721452834739, 3: 157.68364089913908}\n",
      "New error:  182.5593\n",
      "Moved  80  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 65.19724068988474, 0: 178.35234532317543, 1: 230.32434904306004, 3: 173.7574369068436}\n",
      "New error:  167.1687\n",
      "Moved  46  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 116.67164128140998, 0: 184.43110052923552, 1: 250.7786398388496, 3: 105.34985656379476}\n",
      "New error:  161.3079\n",
      "Moved  53  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 92.75212236889644, 0: 183.85795894624087, 1: 218.9259510035378, 3: 243.28798673745973}\n",
      "New error:  194.9036\n",
      "Moved  41  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 83.99216901885832, 0: 181.2585295144192, 1: 234.73321675181208, 3: 88.71498169088206}\n",
      "New error:  153.9322\n",
      "Moved  27  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 132.78093189256253, 0: 116.766112579695, 1: 257.3468258445384, 3: 112.6029495284272}\n",
      "New error:  158.8052\n",
      "Moved  40  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 103.6421723315641, 0: 137.86767099752544, 1: 251.08695657584815, 3: 104.12672369250362}\n",
      "New error:  151.7222\n",
      "Moved  74  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 105.02248325421348, 0: 153.20289806653528, 1: 271.3813348642023, 3: 92.05405732708732}\n",
      "New error:  155.3778\n",
      "Moved  46  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 122.67174965144575, 0: 123.54242272797683, 1: 258.6880452776684, 3: 97.09717456270852}\n",
      "New error:  148.2224\n",
      "Moved  43  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 134.16932564800095, 0: 147.78312154893754, 1: 246.03093940608917, 3: 144.14035302203703}\n",
      "New error:  167.2542\n",
      "Moved  64  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 144.08274589083254, 0: 134.70791095904795, 1: 236.12386439477694, 3: 143.56314625325626}\n",
      "New error:  163.0219\n",
      "Moved  47  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 205.3213198868749, 0: 146.18269541503977, 1: 244.08836543344535, 3: 108.66944676924642}\n",
      "New error:  170.4828\n",
      "Moved  32  users to another cluster \n",
      "\n",
      "Error per cluster:  {2: 99.13478761685374, 0: 223.23453640372492, 1: 207.1642635575736, 3: 127.395706842196}\n",
      "New error:  170.7102\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(51337, 14)\n",
      "(56004, 14)\n",
      "Error per cluster:  {3: 75.69829161771335, 6: 194.06232403478867, 1: 61.088535250150244, 0: 308.04893921306694, 7: 164.14859198031414, 5: 534.371258703509, 4: 72.12189413253512}\n",
      "New error:  189.8234\n",
      "Moved  111  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 218.91645839454867, 6: 183.1901875631847, 1: 216.29317300180287, 0: 159.70437441312353, 7: 33.1794869241554, 5: 115.30194888359834, 4: 111.05292482822517}\n",
      "New error:  175.7238\n",
      "Moved  54  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 107.84557348199215, 6: 212.99529382234962, 1: 135.76469805036274, 0: 106.6769700628148, 7: 162.0693081891584, 5: 213.04400060246397, 4: 164.5126372624836}\n",
      "New error:  167.6014\n",
      "Moved  118  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 82.50802124353073, 6: 181.67251185618042, 1: 227.71199546115892, 0: 213.6067492603551, 7: 83.7504290764482, 5: 182.6903921294263, 4: 44.78866744734302}\n",
      "New error:  133.5041\n",
      "Moved  48  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 112.47228080028515, 6: 233.05535032173978, 1: 165.31096583533653, 0: 129.7234132626909, 7: 100.75587449284916, 5: 192.03020073698275, 4: 85.47454815753099}\n",
      "New error:  153.871\n",
      "Moved  115  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 112.55557246966666, 6: 331.9843715132414, 1: 89.59540956870278, 0: 120.93199930044322, 7: 92.08758491789739, 5: 135.05939173862075, 4: 131.67272599503858}\n",
      "New error:  168.7889\n",
      "Moved  66  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 99.6393163572553, 6: 221.82244536278878, 1: 140.78859177652996, 0: 56.49554760116189, 7: 91.17721281929728, 5: 136.49558278135103, 4: 191.89518522504386}\n",
      "New error:  139.1172\n",
      "Moved  92  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 216.27606525130795, 6: 253.55422560126203, 1: 211.85979238938552, 0: 129.6388159743089, 7: 89.28933476075859, 5: 159.19230866334456, 4: 83.29479502658039}\n",
      "New error:  170.928\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(60671, 14)\n",
      "(65338, 14)\n",
      "Error per cluster:  {7: 36.12039368614785, 1: 79.81264428920387, 2: 292.0991265228532, 0: 128.81338057641287, 5: 136.34054696236805, 6: 308.8739406100748, 3: 239.75345112453775, 4: 225.69647292435135}\n",
      "New error:  190.3903\n",
      "Moved  86  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 159.01881079727565, 1: 194.13657116350979, 2: 92.41064649813215, 0: 139.89103029234758, 5: 249.79198156314808, 6: 270.2393233205043, 3: 75.07753528819445, 4: 101.9700630886823}\n",
      "New error:  182.5377\n",
      "Moved  65  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 36.97184611794313, 1: 152.58702402809257, 2: 150.92873764451693, 0: 116.64411855618171, 5: 273.36223970719226, 6: 245.59516908052882, 3: 292.41627777184925, 4: 338.5834313229036}\n",
      "New error:  203.7849\n",
      "Moved  95  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 122.97344517429934, 1: 270.92055786868525, 2: 105.73999112801815, 0: 131.01424698729465, 5: 157.88294646366617, 6: 268.4652841796875, 3: 116.32342610051083, 4: 230.62112125623827}\n",
      "New error:  194.3289\n",
      "Moved  88  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 243.79758067389585, 1: 160.83359985239133, 2: 90.6390543683834, 0: 55.732771822287525, 5: 30.180110353611234, 6: 129.88043729341945, 3: 187.41432999361479, 4: 130.69572247247382}\n",
      "New error:  147.7787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved  42  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 121.32479285410426, 1: 176.1006556394241, 2: 90.80708245813209, 0: 120.27189656850679, 5: 86.04914102564103, 6: 414.5708172527377, 3: 179.53082952411356, 4: 263.4144954661834}\n",
      "New error:  159.6148\n",
      "Moved  50  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 326.3993714417645, 1: 174.37214151580795, 2: 148.71923976928838, 0: 96.7286363379038, 5: 110.98405610069861, 6: 41.50636017628205, 3: 128.7814042005252, 4: 342.81244914944597}\n",
      "New error:  182.2487\n",
      "Moved  78  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 75.08630842659922, 1: 197.11569664173717, 2: 87.63188038683772, 0: 90.70933981714535, 5: 163.9028231609765, 6: 558.3244467707154, 3: 279.0608215774704, 4: 80.20956637427562}\n",
      "New error:  154.8893\n",
      "Moved  67  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 149.00578445628005, 1: 397.95890492930846, 2: 83.5845962959427, 0: 91.24945448995774, 5: 180.44801513397047, 6: 113.23508789062498, 3: 141.288436741575, 4: 142.02532134141902}\n",
      "New error:  171.6719\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(70005, 14)\n",
      "(74672, 14)\n",
      "CPU times: user 3min 33s, sys: 10.5 s, total: 3min 44s\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "freq = 'weekly'\n",
    "# FOR TRMF\n",
    "for col in ['random initialization2','random initialization1', 'GMM_Voting','GMM_Pair']:\n",
    "    X = train_weekly.drop(columns = ['V'+str(x) for x in list(range(2,delete_column_limit))])\n",
    "    Y  = test_weekly\n",
    "    reggr = XGBRegressor(n_estimators=100)\n",
    "    #reggr = Ridge()\n",
    "    modl = 'XGBoost'\n",
    "   \n",
    "    clusters, cluster_per_user,parameters,reassignment_iterations= clusterwise_regression_sklearn(X,#.drop(columns =['V'+str(x) for x in list(range(2,817))]),\n",
    "                                                                        Y.drop(columns = 'V1'),\n",
    "                                                                        cluster_column=col,\n",
    "                                                                            columns_wo_TS=['random initialization1','V1','random initialization2', 'GMM_Voting','GMM_Pair'],\n",
    "                                                                            score_function=pinball_loss,\n",
    "                                                                           recalculate_assignment=5,\n",
    "                                                                          horizon = False,\n",
    "                                                                           regr = reggr,\n",
    "                                                                          regr_gridsearch_params = None)\n",
    "    for scorer in [MASE,smape]:\n",
    "        dff_sklearn = final_prediction_after_clusterwise_reg_sklearn(X,\n",
    "                                            Y.set_index( 'V1'),\n",
    "                                            regr = reggr,\n",
    "                                            clusters = clusters,\n",
    "                                            clusters_parameters = parameters,\n",
    "                                            new_clusters_for_users = cluster_per_user,\n",
    "                                            gridsearch_use = None,\n",
    "                                            cluster_column=col,\n",
    "                                            scores_per_cluster = False , #important !False,\n",
    "                                            score_function=scorer,#smape,#pinball_loss,#  (lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "                                            columns_wo_TS=['random initialization1','V1','random initialization2', 'GMM_Voting','GMM_Pair'],\n",
    "                                            horizon = False,\n",
    "\n",
    "                                            dataset = 'M4',\n",
    "                                            model = modl,\n",
    "                                            method = 'clusterwise',\n",
    "                                            frequency = freq)\n",
    "\n",
    "        dff_sklearn['clusterwise_loss'] = 'PINBALL'\n",
    "        if scorer==MASE:\n",
    "            dff_sklearn['final_loss'] = 'MASE'\n",
    "        if scorer==smape:\n",
    "            dff_sklearn['final_loss'] = 'SMAPE'\n",
    "        dff_sklearn['reassignments_iterations'] = reassignment_iterations\n",
    "        res = res.append(dff_sklearn)\n",
    "        print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error per cluster:  {1: 249.9186070443019, 0: 234.33858911635022, 6: 226.44025287800685, 2: 176.87775442196798, 5: 255.95235176744336, 4: 263.4964034720618, 3: 240.47549641570134}\n",
      "New error:  236.3781\n",
      "Moved  83  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 39.13692561687852, 0: 51.12693622796588, 6: 63.14988014232318, 2: 585.104064804056, 5: 44.31067969499242, 4: 28.94812109321283, 3: 54.09085959241898}\n",
      "New error:  236.2512\n",
      "Moved  102  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 3.3475736806207608, 0: 6.284657290768002, 6: 15.054617641600156, 2: 242.93261142060527, 5: 8.3046618816738, 4: 463.23418705645105, 3: 9.570384161159124}\n",
      "New error:  236.2626\n",
      "Moved  121  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 482.34754824573434, 0: 0.0, 6: 0.0, 2: 54.76676118927646, 5: 0.9038914141862682, 4: 103.65249267764979, 3: 0.028648772847189474}\n",
      "New error:  236.2932\n",
      "Moved  116  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 135.10267458450602, 0: 512.6029803670608, 6: 0.0, 2: 5.154261671260144, 5: 0.0, 4: 42.64774165789943, 3: 0.0}\n",
      "New error:  236.2883\n",
      "Moved  115  users to another cluster \n",
      "\n",
      "Error per cluster:  {1: 18.192144756559347, 0: 134.63042052510406, 6: 548.4658158809194, 2: 0.3683396661113352, 5: 0.0, 4: 5.4539941379263706, 3: 0.0}\n",
      "New error:  236.2888\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(42003, 14)\n",
      "(46670, 14)\n",
      "Error per cluster:  {0: 266.8602813735989, 2: 255.05033845617612, 3: 224.35123977557987, 1: 204.76214279056492}\n",
      "New error:  236.4893\n",
      "Moved  66  users to another cluster \n",
      "\n",
      "Error per cluster:  {0: 63.980859961392554, 2: 48.96123658654811, 3: 48.7480176334058, 1: 445.47580387683286}\n",
      "New error:  236.3005\n",
      "Moved  97  users to another cluster \n",
      "\n",
      "Error per cluster:  {0: 10.321590887311805, 2: 7.764615845882717, 3: 424.6866550700157, 1: 123.9459299090544}\n",
      "New error:  236.2763\n",
      "Moved  109  users to another cluster \n",
      "\n",
      "Error per cluster:  {0: 0.0, 2: 508.065662469219, 3: 87.3245663615041, 1: 20.677899272510686}\n",
      "New error:  236.2972\n",
      "Moved  121  users to another cluster \n",
      "\n",
      "Error per cluster:  {0: 439.0007979384251, 2: 166.761246700296, 3: 23.372095691695893, 1: 1.213367596885191}\n",
      "New error:  236.2866\n",
      "Moved  117  users to another cluster \n",
      "\n",
      "Error per cluster:  {0: 92.16994573108936, 2: 20.10326877769108, 3: 0.46395285532492064, 1: 449.1470897388584}\n",
      "New error:  236.2953\n",
      "Moved  128  users to another cluster \n",
      "\n",
      "Error per cluster:  {0: 17.46859815919876, 2: 2.7179165339489866, 3: 422.569845830104, 1: 123.59757474479322}\n",
      "New error:  236.285\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(51337, 14)\n",
      "(56004, 14)\n",
      "Error per cluster:  {3: 404.63159332534656, 6: 219.76492455953002, 1: 97.22870805702142, 0: 244.89731133123325, 7: 139.10685388126615, 5: 200.71783228852013, 4: 344.04872133703907}\n",
      "New error:  236.4704\n",
      "Moved  96  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 50.41434837591266, 6: 43.90000476083113, 1: 693.9697035051245, 0: 84.03458479020979, 7: 21.85801671912342, 5: 47.182931663794896, 4: 34.42362859362858}\n",
      "New error:  236.2386\n",
      "Moved  102  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 4.132821520923422, 6: 7.183744926528734, 1: 354.58085955587416, 0: 18.412373737373745, 7: 418.6125138040852, 5: 0.0, 4: 6.4550116550116465}\n",
      "New error:  236.2557\n",
      "Moved  93  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 0.040332403899304224, 6: 0.40652450056786177, 1: 224.54504376958607, 0: 2.818311318311322, 7: 96.70629993312014, 5: 648.7238608847738, 4: 0.0}\n",
      "New error:  236.2872\n",
      "Moved  119  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 0.0, 6: 0.0, 1: 109.06529034469975, 0: 0.0, 7: 42.35331592451602, 5: 249.80852353049966, 4: 516.2510420807843}\n",
      "New error:  236.2901\n",
      "Moved  103  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 537.8068371002752, 6: 0.0, 1: 2.357253814575686, 0: 0.0, 7: 8.319022586203015, 5: 25.703013484940033, 4: 172.37386905565992}\n",
      "New error:  236.228\n",
      "Moved  118  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 155.70000795306268, 6: 367.64592132989935, 1: 0.0, 0: 0.0, 7: 0.5261082550104126, 5: 9.933355160008977, 4: 84.75504311902151}\n",
      "New error:  236.575\n",
      "Moved  112  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 42.559952441330964, 6: 89.7423727520866, 1: 590.9057080514011, 0: 0.0, 7: 0.0, 5: 1.1846043408702625, 4: 3.0254003781740004}\n",
      "New error:  236.2757\n",
      "Moved  126  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 0.0, 6: 10.566777246056985, 1: 217.0339964920163, 0: 512.8317822357268, 7: 0.0, 5: 0.0, 4: 0.9062165173429837}\n",
      "New error:  236.2861\n",
      "Moved  99  users to another cluster \n",
      "\n",
      "Error per cluster:  {3: 540.8933358803187, 6: 0.0, 1: 93.08719025558236, 0: 154.63043678379321, 7: 0.0, 5: 0.0, 4: 0.16884351757853905}\n",
      "New error:  236.2363\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(60671, 14)\n",
      "(65338, 14)\n",
      "Error per cluster:  {7: 399.800596476139, 1: 178.49407015129253, 2: 296.986140791978, 0: 183.48644786866117, 5: 341.8237509486218, 6: 215.55444719038874, 3: 235.38611503749084, 4: 209.8294911364184}\n",
      "New error:  236.5145\n",
      "Moved  81  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 73.22010074369636, 1: 449.1140514735442, 2: 74.72294334960702, 0: 27.57022827906577, 5: 82.12652118342456, 6: 49.788372825867995, 3: 74.21366413510829, 4: 31.302628500439102}\n",
      "New error:  236.2729\n",
      "Moved  117  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 10.881241524205098, 1: 120.67839781527223, 2: 6.562305177092063, 0: 477.269489448325, 5: 11.804379367491816, 6: 11.223591226618813, 3: 17.967959300225203, 4: 0.28515308392868693}\n",
      "New error:  236.2549\n",
      "Moved  119  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 0.010384390239449624, 1: 11.6147529262723, 2: 0.6147487936063788, 0: 116.58536533163563, 5: 0.303163657704094, 6: 0.9791042724716809, 3: 0.0, 4: 543.4922939357007}\n",
      "New error:  236.3108\n",
      "Moved  111  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 0.0, 1: 0.20463697261118727, 2: 0.05960229878263204, 0: 52.97205250399946, 5: 0.0, 6: 0.0, 3: 561.3030517797398, 4: 197.35196771961054}\n",
      "New error:  236.2655\n",
      "Moved  127  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 561.5602402317895, 1: 0.0, 2: 0.0, 0: 10.191099895268925, 5: 0.0, 6: 0.0, 3: 140.27519033967894, 4: 31.954894408602097}\n",
      "New error:  236.2658\n",
      "Moved  101  users to another cluster \n",
      "\n",
      "Error per cluster:  {7: 216.24551810600911, 1: 431.3715834652307, 2: 0.0, 0: 0.796179898532843, 5: 0.0, 6: 0.0, 3: 51.61053535710291, 4: 8.013452790379771}\n",
      "New error:  236.2577\n",
      "Error has increased. Backtrack to previous iteration\n",
      "(70005, 14)\n",
      "(74672, 14)\n",
      "CPU times: user 2min 45s, sys: 12min 10s, total: 14min 56s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "freq = 'weekly'\n",
    "# FOR TRMF\n",
    "for col in ['random initialization2','random initialization1', 'GMM_Voting','GMM_Pair']:\n",
    "    X = train_weekly.drop(columns = ['V'+str(x) for x in list(range(2,delete_column_limit))])\n",
    "    Y  = test_weekly\n",
    "    clusters, cluster_per_user,parameters,reassignment_iterations = clusterwise_regression(X,#.drop(columns =['V'+str(x) for x in list(range(2,817))]),\n",
    "                                                                    Y.drop(columns = 'V1'),\n",
    "                                                                    cluster_column=col,\n",
    "                                                                        columns_wo_TS=['random initialization1','V1','random initialization2', 'GMM_Voting','GMM_Pair'],\n",
    "                                                                        score_function=pinball_loss,\n",
    "                                                                       recalculate_assignment=(10000))\n",
    "    \n",
    "    for scorer in [MASE,smape]:\n",
    "        dff_trmf = final_prediction_after_clusterwise_reg(X,\n",
    "                                        Y.set_index( 'V1'),\n",
    "                                        step ='before_classifier',\n",
    "                                        clusters = clusters,\n",
    "                                        clusters_parameters = parameters,\n",
    "                                        new_clusters_for_users = cluster_per_user,\n",
    "                                        gridsearch_use = None,\n",
    "                                        cluster_column=col,\n",
    "                                        scores_per_cluster = False , #important !False,\n",
    "                                        score_function=scorer,#smape,#pinball_loss,#  (lambda x,y: math.sqrt(mean_squared_error(x,y)))\n",
    "                                        columns_wo_TS=['random initialization1','V1','random initialization2', 'GMM_Voting','GMM_Pair'],\n",
    "\n",
    "                                        dataset = 'M4',\n",
    "                                        model = 'TRMF',\n",
    "                                        method = 'clusterwise',\n",
    "                                        frequency = freq)\n",
    "        \n",
    "        dff_trmf['clusterwise_loss'] = 'PINBALL'\n",
    "        if scorer==MASE:\n",
    "            dff_trmf['final_loss'] = 'MASE'\n",
    "        if scorer==smape:\n",
    "            dff_trmf['final_loss'] = 'SMAPE'\n",
    "        dff_trmf['reassignments_iterations'] = reassignment_iterations\n",
    "        res = res.append(dff_trmf)\n",
    "        print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def write_df(df: pd.DataFrame,\n",
    "             path: str,\n",
    "             project: str,\n",
    "             step: str,\n",
    "             format: str = 'csv') -> None:\n",
    "\n",
    "\n",
    "    title = path + time.strftime(\"%Y%m%d_%H%M\") + '_' + project + '__' + step + '.' + format\n",
    "\n",
    "    if format == 'csv':\n",
    "        df.to_csv(title)\n",
    "\n",
    "    if format == 'h5':\n",
    "        df.to_hdf(title, step, table=True, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df(res, export,'weekly_xgb_final2',str(train_weekly.shape[0])+'_'+str(delete_column_limit)+'+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
